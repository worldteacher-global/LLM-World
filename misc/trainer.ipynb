{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72b0277-2e44-4572-9ca8-eda2c9de9e4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sample training on 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda48f25-431e-4962-beac-f150df382390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized batch: \n",
      " {'input_ids': tensor([[ 101, 2092, 7592, 2045, 1010, 2129, 1005, 1055, 1996, 4633, 1029,  102],\n",
      "        [ 101, 1045, 2064, 1005, 1056, 3524, 2005, 1996, 5353,  999,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "\n",
      "Batch with labels: \n",
      " {'input_ids': tensor([[ 101, 2092, 7592, 2045, 1010, 2129, 1005, 1055, 1996, 4633, 1029,  102],\n",
      "        [ 101, 1045, 2064, 1005, 1056, 3524, 2005, 1996, 5353,  999,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'labels': tensor([1, 1])}\n",
      "\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "\n",
      " Training loss: \n",
      " tensor(0.4401, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sequence = [\n",
    "    \"Well hello there, how's the weather?\",\n",
    "    \"I can't wait for the weekend!\"\n",
    "]\n",
    "\n",
    "batch = tokenizer(sequence, padding=True, truncation=True, return_tensors='pt')\n",
    "print('Tokenized batch: \\n',batch)\n",
    "\n",
    "## Trainng\n",
    "\n",
    "batch[\"labels\"] = torch.tensor([1,1])\n",
    "print('\\nBatch with labels: \\n',batch)\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "print('\\n',optimizer)\n",
    "\n",
    "loss = model(**batch).loss\n",
    "print(\"\\n Training loss: \\n\", loss)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537d071-8cfa-47cd-9325-9efd2860b211",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60a1136-aecf-4984-b371-f6ab4cb2427d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#GLUE Benchmark (10 different classification tasks) Microsoft Research Paraphrase Corpus\n",
    "\n",
    "data = load_dataset(\"glue\", \"mrpc\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52dd6c6-b02b-4bcb-bb81-b69a3fc57b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data['train']\n",
    "print(train_set[0])\n",
    "\n",
    "## get label mapping\n",
    "\n",
    "train_set.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5dbfe-bb17-4f59-924b-ef9e00c29e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess and turn the text into numnbers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inp = tok(train_set[0]['sentence1'],train_set[0]['sentence2'])\n",
    "print('tokenized input: \\n',inp)\n",
    "\n",
    "print('\\ndecoded tokens: \\n',tok.convert_ids_to_tokens(inp.input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec9f14e-0089-400c-8569-ea31f77b8298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=103, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## on the entier dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tok(\n",
    "    data['train']['sentence1'][:],\n",
    "    data['train']['sentence2'][:], padding=True, truncation=True)\n",
    "inputs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351389cb-e785-408d-b380-a563519a89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding left out, because padding every sentence can be inefficiant\n",
    "def tokenize_fn(example):\n",
    "    return tok(example['sentence1'], example['sentence2'], truncation=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce7d57c-bb4b-449e-b689-a996652f6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speeds tokenization ###########\n",
    "tok_data = data.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685c1486-c803-4323-8c7d-de48d3f1a3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d31f2c-e103-402e-8cad-0232783552fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data['train']['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d140a0-68e1-47ce-b6f9-e7675ccd731e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "647ec113-5fa3-4c0e-880d-d73e177485a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf9bab-c8c7-4e49-b86a-83f69c3e5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## collate function: responsible for putting together samples inside a batch, passed when building a DataLoader.\n",
    "## define a collate fn that will apply the correct amount of padding for the items in a dataset we want batched together\n",
    "## define a collate function that will applky the correct amount of padding to the idems of a batch\n",
    "\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10907f7e-5cd6-435b-b308-5890dcaac19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get samples we would like to batch ignore string columns\n",
    "samples = tok_data['train'][:8]\n",
    "samples = {k:v for k,v in samples.items() if k not in ['idx','sentence1','sentence2']}\n",
    "\n",
    "[len(x) for x in samples['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "74b9a267-7cd0-40f7-b817-36146c2899f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
      "          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
      "          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
      "          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
      "          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9805,  3540, 11514,  2050,  3079, 11282,  2243,  1005,  1055,\n",
      "          2077,  4855,  1996,  4677,  2000,  3647,  4576,  1999,  2687,  2005,\n",
      "          1002,  1016,  1012,  1019,  4551,  1012,   102,  9805,  3540, 11514,\n",
      "          2050,  4149, 11282,  2243,  1005,  1055,  1999,  2786,  2005,  1002,\n",
      "          6353,  2509,  2454,  1998,  2853,  2009,  2000,  3647,  4576,  2005,\n",
      "          1002,  1015,  1012,  1022,  4551,  1999,  2687,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2027,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  2006,\n",
      "          2238,  2184,  1010,  5378,  1996,  6636,  2005,  5096,  1010,  2002,\n",
      "          2794,  1012,   102,  2006,  2238,  2184,  1010,  1996,  2911,  1005,\n",
      "          1055,  5608,  2018,  2405,  2019, 15147,  2006,  1996,  4274,  1010,\n",
      "          5378,  1996, 14792,  2005,  5096,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2105,  6021, 19481, 13938,  2102,  1010, 21628,  6661,  2020,\n",
      "          2039,  2539, 16653,  1010,  2030,  1018,  1012,  1018,  1003,  1010,\n",
      "          2012,  1037,  1002,  1018,  1012,  5179,  1010,  2383,  3041,  2275,\n",
      "          1037,  2501,  2152,  1997,  1037,  1002,  1018,  1012,  5401,  1012,\n",
      "           102, 21628,  6661,  5598,  2322, 16653,  1010,  2030,  1018,  1012,\n",
      "          1020,  1003,  1010,  2000,  2275,  1037,  2501,  5494,  2152,  2012,\n",
      "          1037,  1002,  1018,  1012,  5401,  1012,   102],\n",
      "        [  101,  1996,  4518,  3123,  1002,  1016,  1012,  2340,  1010,  2030,\n",
      "          2055,  2340,  3867,  1010,  2000,  2485,  5958,  2012,  1002,  2538,\n",
      "          1012,  4868,  2006,  1996,  2047,  2259,  4518,  3863,  1012,   102,\n",
      "         18720,  1004,  1041, 13058,  1012,  6661,  5598,  1002,  1015,  1012,\n",
      "          6191,  2030,  1022,  3867,  2000,  1002,  2538,  1012,  6021,  2006,\n",
      "          1996,  2047,  2259,  4518,  3863,  2006,  5958,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6599,  1999,  1996,  2034,  4284,  1997,  1996,  2095,  3333,\n",
      "          2321,  3867,  2013,  1996,  2168,  2558,  1037,  2095,  3041,  1012,\n",
      "           102,  2007,  1996,  9446,  5689,  2058,  5954,  1005,  1055,  2194,\n",
      "          1010,  6599,  1996,  2034,  4284,  1997,  1996,  2095,  3333,  2321,\n",
      "          3867,  2013,  1996,  2168,  2558,  1037,  2095,  3041,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996, 17235,  2850,  4160,  2018,  1037,  4882,  5114,  1997,\n",
      "          2459,  1012,  2676,  1010,  2030,  1015,  1012,  1016,  3867,  1010,\n",
      "          5494,  2012,  1015,  1010, 19611,  1012,  2321,  2006,  5958,  1012,\n",
      "           102,  1996,  6627,  1011, 17958, 17235,  2850,  4160, 12490,  1012,\n",
      "         11814,  2594, 24356,  2382,  1012,  4805,  2685,  1010,  2030,  1016,\n",
      "          1012,  5840,  3867,  1010,  2000,  1015,  1010, 19611,  1012,  2321,\n",
      "          1012,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  4966,  1011, 10507,  2050,  2059, 12068,  2000,  1996,\n",
      "          2110,  4259,  2457,  1012,   102,  1996,  4966, 10507,  2050, 12068,\n",
      "          2008,  3247,  2000,  1996,  1057,  1012,  1055,  1012,  4259,  2457,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "## check to confirm padding is ocurring correcly\n",
    "\n",
    "collated_sample = data_collator(samples)\n",
    "print(collated_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4b64f68a-aedb-4165-a37d-a060c26ed3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v.shape for k,v in collated_sample.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088bb0c-ed13-4c56-91a8-c70614a08176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8ad266-fbd9-451a-bbc1-27a4a5c50d43",
   "metadata": {},
   "source": [
    "## Fine-tuning Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d74541-3a66-4195-9f59-e01b1dd6a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRL_GRADIO_ENABLED\"] = \"0\"      # disables Gradio UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2197fb45-2334-47cf-9836-2b69a1619296",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainer class helps fine-tune any pretrained models with modern best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f0bd3c-851a-4ad3-93e9-6504260f1cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## define a TrainingArguments class to contain all the hyperparameters the Trainer will use for training/evaluation\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# help(TrainingArguments)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='misc/files/training_out/',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54fcec2b-94c4-4f8d-b7ae-9b5c951cd5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## define the model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06c03f7a-6795-4669-bb2f-4c498bfb8952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ba85d-a097-4369-bb3f-b89488200b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a348e3aa-db24-416e-a446-ec2f5972c677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Trainer doesn't work with docker instance and private network, tries to launch gradio on localhost and fails to connect.\n",
    "from transformers import Trainer, DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=train_args,\n",
    "    train_dataset=tok_data['train'],\n",
    "    eval_dataset=tok_data['validation'],\n",
    "    data_collator=data_collator,    \n",
    "    processing_class=tok # specifies the tokenizer to use for processing when included, by default data_collator will be DataCollatorWithPadding and can be left out\n",
    ")\n",
    "\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=classifier,\n",
    "#     args=train_args,\n",
    "#     train_dataset=tok_data['train'],\n",
    "#     eval_dataset=tok_data['validation'],\n",
    "#     data_collator=data_collator,\n",
    "#     processing_class=tok\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56945d00-a0a1-451e-a188-b8370246c39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(SFTTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea32756-af2c-4366-ac35-6eb9a658a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 00:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=115, training_loss=0.5605452164359714, metrics={'train_runtime': 30.0457, 'train_samples_per_second': 122.081, 'train_steps_per_second': 3.828, 'total_flos': 150071968200960.0, 'train_loss': 0.5605452164359714, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fine-tune the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007602cc-30fa-4d71-ad02-9f2586086c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
