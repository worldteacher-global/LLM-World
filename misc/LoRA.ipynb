{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affdee44-deea-42d8-872e-1a5f1aaf1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "#GLUE Benchmark (10 different classification tasks) Microsoft Research Paraphrase Corpus\n",
    "\n",
    "data = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "## on the entier dataset\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tok(\n",
    "    data['train']['sentence1'][:],\n",
    "    data['train']['sentence2'][:], padding=True, truncation=True)\n",
    "\n",
    "## padding left out, because padding every sentence can be inefficiant\n",
    "def tokenize_fn(example):\n",
    "    return tok(example['sentence1'], example['sentence2'], truncation=True)\n",
    "    \n",
    "tok_data = data.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2940a783-3021-4a29-b02f-7439de504609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/opt-350m\n"
     ]
    }
   ],
   "source": [
    "## Low-Rank Adaptation\n",
    "### Adapters can be loaded onto a pretrained model wiht (load_adapter())\n",
    "### Set the active adapter weights with (set_adapter())\n",
    "### Return base model (unload())\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\n",
    "print(config.base_model_name_or_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(model,\"ybelkada/opt-350m-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fed366-0465-4648-a5b2-6e1128c9cf48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a393d55-23c7-49ed-84b3-176c3142f670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6812e723-1b98-407c-9c8a-7bb4388a27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRL_GRADIO_ENABLED\"] = \"0\"  \n",
    "\n",
    "## trl provides integration with LoRA adapters through PEFT library\n",
    "\n",
    "# 1. Define the LoRA confuguration (rank, alpha, dropout)\n",
    "# 2. Create the SFTTrainer with PEFT config\n",
    "# 3. Train and save adapter weights\n",
    "\n",
    "# 1.\n",
    "## Define LoRA configuration\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "rank = 5 # [4-32] smaller = more cmpression (dimension for LoRA update matrices)\n",
    "lora_alpha = 8 # [2x(rank]) higher = stronger adaptation (scaling factor) \"how much of the pretrained model's behavior is modified by newlwy added low-rank updates\" modle output influence\n",
    "lora_dropout = 0.05 # [0.05-1] helps prevent overfitting \"probability\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias='none',\n",
    "    target_modules='all-linear', # which modules to apply LoRA to\n",
    "    task_type='CAUSAL_LM', # task type for model arch \n",
    "    \n",
    ") \n",
    "\n",
    "# 2.\n",
    "## Create Trainer\n",
    "from trl import SFTTrainer\n",
    "arguments = TrainingArguments(\n",
    "    output_dir='misc/files/training_out/',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=tok_data['train'],\n",
    "    peft_config=peft_config,\n",
    "    # max_seq_length=max_seq_length,\n",
    "    processing_class=tok\n",
    ")\n",
    "\n",
    "# 3.\n",
    "## Save models\n",
    "# model.save_pretrained(\"path/to/folder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d983c-881b-4de7-9aed-bbb316a25f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66cf69a9-2765-4bd5-849f-96293ab365bf",
   "metadata": {},
   "source": [
    "# Merging Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974584b4-4e51-48f8-9afb-49b33396e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training LoRA adapter you can merge the adapter weights back into the base model:\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1. Load Base model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained('openai/gpt-oss-20b', torch_dtype=torch.float16, device_map='auto')\n",
    "\n",
    "# 2. Load PEFT model with adapter\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"path/to/adapter\", torch_dtype=torch.float16) # prep-model to beable to load adapter (personalized models)\n",
    "\n",
    "# 3. Merge adapter weights with base model\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "\n",
    "## save both model and tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai/gpt-oss-20b')\n",
    "merged_model.save_pretrained('files/training_out/')\n",
    "tokenizer.save_pretrained('files/training_out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31c19c-e489-4a0c-acd9-f4d5c91bc82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
