{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a791a2d-94b9-4e2e-a393-c89b1705c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ce5b218-ac98-4cba-a8f7-f9559179be4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[54609,  1078,  6602, 24223,   382,  2827],\n",
       "        [   40,  4128,  1299,   532,  2760, 10664]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'openai/gpt-oss-20b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "raw_input = [\n",
    "    \"Learning about tokenizers is fun\",\n",
    "    \"I don't like OOM errors\"\n",
    "]\n",
    "\n",
    "inp = tokenizer(raw_input, padding=True, truncation=True, return_tensors='pt')\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b15bec-fc2f-41f3-b710-48d1272a9016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c391600-e27d-422c-bc85-21c768808938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [13347, 382, 41482, 3521, 30], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "what is today's date?\n"
     ]
    }
   ],
   "source": [
    "inp = tokenizer(\"what is today's date?\")\n",
    "print(inp)\n",
    "print(tokenizer.decode(inp['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778507ca-c0a2-477c-b771-d993ab3dd7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download pretrained model\n",
    "# *Model (retrieve the hidden states)\n",
    "# *ForCausalLM\n",
    "# *ForMaskedLM\n",
    "# *ForMultipleChoice\n",
    "# *ForQuestionAnswering\n",
    "# *ForSequenceClassification\n",
    "# *ForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5334c0e-976b-4c56-9c2d-f4a53e01156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a32def-caf8-4e0b-9bd1-bfae4cd80ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa48085a78246c990364d35d9b928d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943489e16bbc4893b4dcf5d673166315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_card = 'openai/gpt-oss-20b'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_card, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d143d7-6ce0-4480-8e80-bf3923ce630c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GptOssModel(\n",
       "  (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x GptOssDecoderLayer(\n",
       "      (self_attn): GptOssAttention(\n",
       "        (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
       "        (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "        (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "        (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
       "      )\n",
       "      (mlp): GptOssMLP(\n",
       "        (router): GptOssTopKRouter()\n",
       "        (experts): Mxfp4GptOssExperts()\n",
       "      )\n",
       "      (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "      (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "  (rotary_emb): GptOssRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1df34468-120f-4e09-8456-4e449f207fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoeModelOutputWithPast(last_hidden_state=tensor([[[ 1.2188e+00, -4.2500e+00,  4.1250e+00,  ..., -4.0312e+00,\n",
       "           9.1250e+00, -1.8375e+01],\n",
       "         [-5.1875e+00,  2.4219e+00, -1.4062e+00,  ..., -1.5469e+00,\n",
       "           3.3203e-01, -1.9125e+01],\n",
       "         [-8.5156e-01, -1.0703e+00,  3.8125e+00,  ...,  2.5312e+00,\n",
       "           3.7188e+00, -1.9062e+00],\n",
       "         [-1.5625e+00, -1.6406e+00,  3.0469e+00,  ...,  9.9219e-01,\n",
       "           4.8750e+00, -1.1938e+01],\n",
       "         [-8.8750e+00, -2.7148e-01, -8.4375e-01,  ..., -1.8203e+00,\n",
       "           3.0312e+00,  4.0000e+00],\n",
       "         [-5.4688e+00,  3.7305e-01,  3.0273e-01,  ..., -1.3281e+00,\n",
       "          -1.0312e+00, -3.6250e+00]],\n",
       "\n",
       "        [[-7.5625e+00, -3.0312e+00, -9.2578e-01,  ...,  9.6875e-01,\n",
       "          -3.2344e+00, -3.4531e+00],\n",
       "         [ 1.4562e+01,  4.6875e+00, -5.1953e-01,  ..., -2.6719e+00,\n",
       "           2.4531e+00, -1.8281e+00],\n",
       "         [-8.3125e+00,  2.9531e+00,  2.5781e+00,  ..., -4.1250e+00,\n",
       "           1.9609e+00, -1.3625e+01],\n",
       "         [-1.2375e+01,  1.5137e-01,  1.1719e+00,  ..., -8.2812e-01,\n",
       "          -2.8125e-01, -3.2344e+00],\n",
       "         [ 6.2561e-03, -3.9844e+00,  8.1250e+00,  ...,  2.2656e+00,\n",
       "           1.6328e+00, -1.6000e+01],\n",
       "         [-6.6875e+00,  6.1719e-01,  1.8750e-01,  ..., -2.5625e+00,\n",
       "          -1.9609e+00, -7.7734e-01]]], dtype=torch.bfloat16,\n",
       "       grad_fn=<ToCopyBackward0>), past_key_values=DynamicCache(layers=[<transformers.cache_utils.DynamicLayer object at 0x7ff0b0a5e240>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0ac23f0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0ace540>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acefc0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acd9a0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0ace210>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acdf10>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acf470>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acda60>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0accd70>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acdaf0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acf9b0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acf920>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acfbc0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acfcb0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acfef0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acd8b0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acca40>, <transformers.cache_utils.DynamicLayer object at 0x7ff10c59b950>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0ac31d0>, <transformers.cache_utils.DynamicLayer object at 0x7ff10c56d7c0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0ace8a0>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0acea80>, <transformers.cache_utils.DynamicLayer object at 0x7ff0b0ace6f0>]), hidden_states=None, attentions=None, router_logits=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b183aa6-2011-49b1-8440-260e10487250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2880])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2188e+00, -4.2500e+00,  4.1250e+00,  ..., -4.0312e+00,\n",
       "           9.1250e+00, -1.8375e+01],\n",
       "         [-5.1875e+00,  2.4219e+00, -1.4062e+00,  ..., -1.5469e+00,\n",
       "           3.3203e-01, -1.9125e+01],\n",
       "         [-8.5156e-01, -1.0703e+00,  3.8125e+00,  ...,  2.5312e+00,\n",
       "           3.7188e+00, -1.9062e+00],\n",
       "         [-1.5625e+00, -1.6406e+00,  3.0469e+00,  ...,  9.9219e-01,\n",
       "           4.8750e+00, -1.1938e+01],\n",
       "         [-8.8750e+00, -2.7148e-01, -8.4375e-01,  ..., -1.8203e+00,\n",
       "           3.0312e+00,  4.0000e+00],\n",
       "         [-5.4688e+00,  3.7305e-01,  3.0273e-01,  ..., -1.3281e+00,\n",
       "          -1.0312e+00, -3.6250e+00]],\n",
       "\n",
       "        [[-7.5625e+00, -3.0312e+00, -9.2578e-01,  ...,  9.6875e-01,\n",
       "          -3.2344e+00, -3.4531e+00],\n",
       "         [ 1.4562e+01,  4.6875e+00, -5.1953e-01,  ..., -2.6719e+00,\n",
       "           2.4531e+00, -1.8281e+00],\n",
       "         [-8.3125e+00,  2.9531e+00,  2.5781e+00,  ..., -4.1250e+00,\n",
       "           1.9609e+00, -1.3625e+01],\n",
       "         [-1.2375e+01,  1.5137e-01,  1.1719e+00,  ..., -8.2812e-01,\n",
       "          -2.8125e-01, -3.2344e+00],\n",
       "         [ 6.2561e-03, -3.9844e+00,  8.1250e+00,  ...,  2.2656e+00,\n",
       "           1.6328e+00, -1.6000e+01],\n",
       "         [-6.6875e+00,  6.1719e-01,  1.8750e-01,  ..., -2.5625e+00,\n",
       "          -1.9609e+00, -7.7734e-01]]], dtype=torch.bfloat16,\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out.last_hidden_state.shape)\n",
    "out.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1d1d710-c2a6-4cab-bdfb-01b0fe74cea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e946842-efec-4b72-bd63-852a8f457a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14fe7756-1b92-4010-8870-cf3b7eb5d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  4083,  2055, 19204, 17629,  2015,  2003,  4569,   102,     0],\n",
      "        [  101,  1045,  2123,  1005,  1056,  2066,  1051,  5358, 10697,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[CLS] learning about tokenizers is fun [SEP] [PAD]\n",
      "[CLS] i don't like oom errors [SEP]\n"
     ]
    }
   ],
   "source": [
    "model_card = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_card)\n",
    "\n",
    "raw_input = [\n",
    "    \"Learning about tokenizers is fun\",\n",
    "    \"I don't like OOM errors\"\n",
    "]\n",
    "\n",
    "inp = tok(raw_input, padding=True, truncation=True, return_tensors='pt')\n",
    "print(inp)\n",
    "\n",
    "print(tok.decode(inp.input_ids[0]))\n",
    "print(tok.decode(inp.input_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2e37db-d667-4853-8215-db5471fc82a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce39980142543c1aa01c63ff4d45a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "out = model(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0f6690-03a9-4cc2-95a7-159d84eb2bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.2268,  4.5430],\n",
       "        [ 2.5654, -2.1671]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits (raw unnormalized scores to be converted into probability)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb1c970-8387-4ea7-a8a7-9bed647058ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5533e-04, 9.9984e-01],\n",
       "        [9.9127e-01, 8.7273e-03]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pred = torch.nn.functional.softmax(out.logits, dim=-1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be7e9d79-6833-48e5-b9ed-a7983a204688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee1e52-78eb-4718-bf95-5757580a236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "# Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6265d-5524-4fa8-bef6-6c3713dc9527",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be6d11c-acc7-4c92-afbf-ec2afcdf3fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d12709d7704759afe7889c5aaed80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374972d0b08443cfb2fd5aada2fcc00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9999ebb-7f99-44b7-a64b-c79aab75b74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b7f5b4-d572-4253-9d9f-b10967a66acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/misc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bd5af-462a-45fa-9c6e-aaa5129e74b6",
   "metadata": {},
   "source": [
    "## saving model weights and arch config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb68a995-e9bc-4b70-ab2b-3d7747bf2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model weights and architecture config\n",
    "\n",
    "model.save_pretrained('/app/misc/model_card/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec67b5e-8613-45a7-aa21-2991220d106c",
   "metadata": {},
   "source": [
    "## Multiple sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd48b185-49bd-4dc4-bd6f-a843ba9ff29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87678b21-5ab4-47b7-965a-4c05d8fc8938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'this', 'course', 'my', 'whole', 'life', '.']\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 2023, 2607, 2026, 2878, 2166, 1012]\n",
      "tensor([[1045, 1005, 2310, 2042, 3403, 2005, 2023, 2607, 2026, 2878, 2166, 1012],\n",
      "        [1045, 1005, 2310, 2042, 3403, 2005, 2023, 2607, 2026, 2878, 2166, 1012]])\n",
      "tensor([[-2.7220,  2.7847],\n",
      "        [-2.7220,  2.7847]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_weight = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_weight)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_weight)\n",
    "\n",
    "sequence = \"I've been waiting for this course my  whole life.\"\n",
    "\n",
    "tokens = tok.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids = tok.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "## Batched input\n",
    "input_ids = torch.tensor([ids, ids]) ## <---\n",
    "print(input_ids)\n",
    "\n",
    "out = model(input_ids)\n",
    "print(out.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "855ec588-c582-4a6e-9294-5368a2df3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200, 200, 200], [200, 200]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[200, 200, 200], [200, 200, 100]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Padding (Manual)\n",
    "\n",
    "batch = [[200,200,200], [200,200]]\n",
    "print(batch)\n",
    "padding_id = 100\n",
    "\n",
    "batch[1].append(padding_id)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47857c18-9300-4b79-b2a1-bccf446d601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200, 200, 200], [200, 200]]\n",
      "[[200, 200, 200], [200, 200, 0]]\n"
     ]
    }
   ],
   "source": [
    "## Padding (tokenizer class value)\n",
    "sequence1_ids = [[200,200,200]]\n",
    "sequence2_ids = [[200,200]]\n",
    "batch_ids = [\n",
    "    [200,200,200], \n",
    "    [200,200]]\n",
    "print(batch_ids)\n",
    "batch_ids[1].append(tok.pad_token_id)\n",
    "print(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0350a7-4e3b-40f8-b4b3-56c1f33fd165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## convert to tensor\n",
    "seq1_tensor = torch.tensor(sequence1_ids)\n",
    "seq2_tensor = torch.tensor(sequence2_ids)\n",
    "batch_id_tensor = torch.tensor(batch_ids)\n",
    "print(seq1_tensor.shape)\n",
    "print(seq2_tensor.shape)\n",
    "print(batch_id_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76b3d901-8779-4298-bef8-27dcbdd83556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence 1 logits: tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence 2 logits: tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "batch sequence logits: tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Run through classifier and output logits\n",
    "print('sequence 1 logits:', model(seq1_tensor).logits)\n",
    "print('sequence 2 logits:', model(seq2_tensor).logits)\n",
    "print('batch sequence logits:', model(batch_id_tensor).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b53f238-8ff7-4224-9e4f-be26c1852e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1], [1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# padding value was used during self-attention. to ignroe the padding, must apply an attention mask\n",
    "\n",
    "attention_mask = [[1,1,1], [1,1,0]]\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd3970f5-bf7a-4146-a5c7-48e798f5d264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5694, -1.3895],\n",
       "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch_id_tensor, attention_mask=torch.tensor(attention_mask)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b35fdf15-cafe-4018-8614-842c9fc5fd88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AutoTokenizer in module transformers.models.auto.tokenization_auto:\n",
      "\n",
      "class AutoTokenizer(builtins.object)\n",
      " |  This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when\n",
      " |  created with the [`AutoTokenizer.from_pretrained`] class method.\n",
      " |\n",
      " |  This class cannot be instantiated directly using `__init__()` (throws an error).\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      " |      Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\n",
      " |\n",
      " |      The tokenizer class to instantiate is selected based on the `model_type` property of the config object (either\n",
      " |      passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
      " |      falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
      " |\n",
      " |          - **aimv2** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (AIMv2 model)\n",
      " |          - **albert** -- [`AlbertTokenizerFast`] (ALBERT model)\n",
      " |          - **align** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ALIGN model)\n",
      " |          - **arcee** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Arcee model)\n",
      " |          - **aria** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Aria model)\n",
      " |          - **aya_vision** -- [`CohereTokenizerFast`] (AyaVision model)\n",
      " |          - **bark** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Bark model)\n",
      " |          - **bart** -- [`BartTokenizer`] or [`BartTokenizerFast`] (BART model)\n",
      " |          - **barthez** -- [`BarthezTokenizerFast`] (BARThez model)\n",
      " |          - **bartpho** -- [`BartphoTokenizer`] (BARTpho model)\n",
      " |          - **bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BERT model)\n",
      " |          - **bert-generation** --  (Bert Generation model)\n",
      " |          - **bert-japanese** -- [`BertJapaneseTokenizer`] (BertJapanese model)\n",
      " |          - **bertweet** -- [`BertweetTokenizer`] (BERTweet model)\n",
      " |          - **big_bird** -- [`BigBirdTokenizerFast`] (BigBird model)\n",
      " |          - **bigbird_pegasus** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (BigBird-Pegasus model)\n",
      " |          - **biogpt** -- [`BioGptTokenizer`] (BioGpt model)\n",
      " |          - **bitnet** -- [`PreTrainedTokenizerFast`] (BitNet model)\n",
      " |          - **blenderbot** -- [`BlenderbotTokenizer`] or [`BlenderbotTokenizerFast`] (Blenderbot model)\n",
      " |          - **blenderbot-small** -- [`BlenderbotSmallTokenizer`] (BlenderbotSmall model)\n",
      " |          - **blip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BLIP model)\n",
      " |          - **blip-2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (BLIP-2 model)\n",
      " |          - **bloom** -- [`BloomTokenizerFast`] (BLOOM model)\n",
      " |          - **bridgetower** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (BridgeTower model)\n",
      " |          - **bros** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BROS model)\n",
      " |          - **byt5** -- [`ByT5Tokenizer`] (ByT5 model)\n",
      " |          - **camembert** -- [`CamembertTokenizerFast`] (CamemBERT model)\n",
      " |          - **canine** -- [`CanineTokenizer`] (CANINE model)\n",
      " |          - **chameleon** -- [`LlamaTokenizerFast`] (Chameleon model)\n",
      " |          - **chinese_clip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Chinese-CLIP model)\n",
      " |          - **clap** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (CLAP model)\n",
      " |          - **clip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIP model)\n",
      " |          - **clipseg** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIPSeg model)\n",
      " |          - **clvp** -- [`ClvpTokenizer`] (CLVP model)\n",
      " |          - **code_llama** -- [`CodeLlamaTokenizerFast`] (CodeLlama model)\n",
      " |          - **codegen** -- [`CodeGenTokenizer`] or [`CodeGenTokenizerFast`] (CodeGen model)\n",
      " |          - **cohere** -- [`CohereTokenizerFast`] (Cohere model)\n",
      " |          - **cohere2** -- [`CohereTokenizerFast`] (Cohere2 model)\n",
      " |          - **colpali** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (ColPali model)\n",
      " |          - **colqwen2** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (ColQwen2 model)\n",
      " |          - **convbert** -- [`ConvBertTokenizer`] or [`ConvBertTokenizerFast`] (ConvBERT model)\n",
      " |          - **cpm** -- [`CpmTokenizerFast`] (CPM model)\n",
      " |          - **cpmant** -- [`CpmAntTokenizer`] (CPM-Ant model)\n",
      " |          - **ctrl** -- [`CTRLTokenizer`] (CTRL model)\n",
      " |          - **data2vec-audio** -- [`Wav2Vec2CTCTokenizer`] (Data2VecAudio model)\n",
      " |          - **data2vec-text** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (Data2VecText model)\n",
      " |          - **dbrx** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (DBRX model)\n",
      " |          - **deberta** -- [`DebertaTokenizer`] or [`DebertaTokenizerFast`] (DeBERTa model)\n",
      " |          - **deberta-v2** -- [`DebertaV2TokenizerFast`] (DeBERTa-v2 model)\n",
      " |          - **deepseek_v2** -- [`LlamaTokenizerFast`] (DeepSeek-V2 model)\n",
      " |          - **deepseek_v3** -- [`LlamaTokenizerFast`] (DeepSeek-V3 model)\n",
      " |          - **deepseek_vl** -- [`LlamaTokenizerFast`] (DeepseekVL model)\n",
      " |          - **deepseek_vl_hybrid** -- [`LlamaTokenizerFast`] (DeepseekVLHybrid model)\n",
      " |          - **dia** -- [`DiaTokenizer`] (Dia model)\n",
      " |          - **diffllama** -- [`LlamaTokenizerFast`] (DiffLlama model)\n",
      " |          - **distilbert** -- [`DistilBertTokenizer`] or [`DistilBertTokenizerFast`] (DistilBERT model)\n",
      " |          - **dpr** -- [`DPRQuestionEncoderTokenizer`] or [`DPRQuestionEncoderTokenizerFast`] (DPR model)\n",
      " |          - **electra** -- [`ElectraTokenizer`] or [`ElectraTokenizerFast`] (ELECTRA model)\n",
      " |          - **emu3** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (Emu3 model)\n",
      " |          - **ernie** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ERNIE model)\n",
      " |          - **ernie4_5** -- [`LlamaTokenizerFast`] (Ernie4_5 model)\n",
      " |          - **ernie4_5_moe** -- [`LlamaTokenizerFast`] (Ernie4_5_MoE model)\n",
      " |          - **ernie_m** --  (ErnieM model)\n",
      " |          - **esm** -- [`EsmTokenizer`] (ESM model)\n",
      " |          - **exaone4** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (EXAONE-4.0 model)\n",
      " |          - **falcon** -- [`PreTrainedTokenizerFast`] (Falcon model)\n",
      " |          - **falcon_mamba** -- [`GPTNeoXTokenizerFast`] (FalconMamba model)\n",
      " |          - **fastspeech2_conformer** --  (FastSpeech2Conformer model)\n",
      " |          - **flaubert** -- [`FlaubertTokenizer`] (FlauBERT model)\n",
      " |          - **fnet** -- [`FNetTokenizer`] or [`FNetTokenizerFast`] (FNet model)\n",
      " |          - **fsmt** -- [`FSMTTokenizer`] (FairSeq Machine-Translation model)\n",
      " |          - **funnel** -- [`FunnelTokenizer`] or [`FunnelTokenizerFast`] (Funnel Transformer model)\n",
      " |          - **gemma** -- [`GemmaTokenizerFast`] (Gemma model)\n",
      " |          - **gemma2** -- [`GemmaTokenizerFast`] (Gemma2 model)\n",
      " |          - **gemma3** -- [`GemmaTokenizerFast`] (Gemma3ForConditionalGeneration model)\n",
      " |          - **gemma3_text** -- [`GemmaTokenizerFast`] (Gemma3ForCausalLM model)\n",
      " |          - **gemma3n** -- [`GemmaTokenizerFast`] (Gemma3nForConditionalGeneration model)\n",
      " |          - **gemma3n_text** -- [`GemmaTokenizerFast`] (Gemma3nForCausalLM model)\n",
      " |          - **git** -- [`BertTokenizer`] or [`BertTokenizerFast`] (GIT model)\n",
      " |          - **glm** -- [`PreTrainedTokenizerFast`] (GLM model)\n",
      " |          - **glm4** -- [`PreTrainedTokenizerFast`] (GLM4 model)\n",
      " |          - **glm4_moe** -- [`PreTrainedTokenizerFast`] (Glm4MoE model)\n",
      " |          - **glm4v** -- [`PreTrainedTokenizerFast`] (GLM4V model)\n",
      " |          - **gpt-sw3** --  (GPT-Sw3 model)\n",
      " |          - **gpt2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (OpenAI GPT-2 model)\n",
      " |          - **gpt_bigcode** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPTBigCode model)\n",
      " |          - **gpt_neo** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT Neo model)\n",
      " |          - **gpt_neox** -- [`GPTNeoXTokenizerFast`] (GPT NeoX model)\n",
      " |          - **gpt_neox_japanese** -- [`GPTNeoXJapaneseTokenizer`] (GPT NeoX Japanese model)\n",
      " |          - **gpt_oss** -- [`PreTrainedTokenizerFast`] (GptOss model)\n",
      " |          - **gptj** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT-J model)\n",
      " |          - **gptsan-japanese** -- [`GPTSanJapaneseTokenizer`] (GPTSAN-japanese model)\n",
      " |          - **granite** -- [`GPT2Tokenizer`] (Granite model)\n",
      " |          - **granitemoe** -- [`GPT2Tokenizer`] (GraniteMoeMoe model)\n",
      " |          - **granitemoehybrid** -- [`GPT2Tokenizer`] (GraniteMoeHybrid model)\n",
      " |          - **granitemoeshared** -- [`GPT2Tokenizer`] (GraniteMoeSharedMoe model)\n",
      " |          - **grounding-dino** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Grounding DINO model)\n",
      " |          - **groupvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (GroupViT model)\n",
      " |          - **helium** -- [`PreTrainedTokenizerFast`] (Helium model)\n",
      " |          - **herbert** -- [`HerbertTokenizer`] or [`HerbertTokenizerFast`] (HerBERT model)\n",
      " |          - **hubert** -- [`Wav2Vec2CTCTokenizer`] (Hubert model)\n",
      " |          - **ibert** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (I-BERT model)\n",
      " |          - **idefics** -- [`LlamaTokenizerFast`] (IDEFICS model)\n",
      " |          - **idefics2** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Idefics2 model)\n",
      " |          - **idefics3** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Idefics3 model)\n",
      " |          - **instructblip** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (InstructBLIP model)\n",
      " |          - **instructblipvideo** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (InstructBlipVideo model)\n",
      " |          - **internvl** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (InternVL model)\n",
      " |          - **jamba** -- [`LlamaTokenizerFast`] (Jamba model)\n",
      " |          - **janus** -- [`LlamaTokenizerFast`] (Janus model)\n",
      " |          - **jetmoe** -- [`LlamaTokenizerFast`] (JetMoe model)\n",
      " |          - **jukebox** -- [`JukeboxTokenizer`] (Jukebox model)\n",
      " |          - **kosmos-2** -- [`XLMRobertaTokenizerFast`] (KOSMOS-2 model)\n",
      " |          - **layoutlm** -- [`LayoutLMTokenizer`] or [`LayoutLMTokenizerFast`] (LayoutLM model)\n",
      " |          - **layoutlmv2** -- [`LayoutLMv2Tokenizer`] or [`LayoutLMv2TokenizerFast`] (LayoutLMv2 model)\n",
      " |          - **layoutlmv3** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LayoutLMv3 model)\n",
      " |          - **layoutxlm** -- [`LayoutXLMTokenizer`] or [`LayoutXLMTokenizerFast`] (LayoutXLM model)\n",
      " |          - **led** -- [`LEDTokenizer`] or [`LEDTokenizerFast`] (LED model)\n",
      " |          - **lilt** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LiLT model)\n",
      " |          - **llama** -- [`LlamaTokenizerFast`] (LLaMA model)\n",
      " |          - **llama4** -- [`LlamaTokenizerFast`] (Llama4 model)\n",
      " |          - **llama4_text** -- [`LlamaTokenizerFast`] (Llama4ForCausalLM model)\n",
      " |          - **llava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVa model)\n",
      " |          - **llava_next** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVA-NeXT model)\n",
      " |          - **llava_next_video** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVa-NeXT-Video model)\n",
      " |          - **llava_onevision** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (LLaVA-Onevision model)\n",
      " |          - **longformer** -- [`LongformerTokenizer`] or [`LongformerTokenizerFast`] (Longformer model)\n",
      " |          - **longt5** -- [`T5TokenizerFast`] (LongT5 model)\n",
      " |          - **luke** -- [`LukeTokenizer`] (LUKE model)\n",
      " |          - **lxmert** -- [`LxmertTokenizer`] or [`LxmertTokenizerFast`] (LXMERT model)\n",
      " |          - **m2m_100** --  (M2M100 model)\n",
      " |          - **mamba** -- [`GPTNeoXTokenizerFast`] (Mamba model)\n",
      " |          - **mamba2** -- [`GPTNeoXTokenizerFast`] (mamba2 model)\n",
      " |          - **marian** --  (Marian model)\n",
      " |          - **mbart** -- [`MBartTokenizerFast`] (mBART model)\n",
      " |          - **mbart50** -- [`MBart50TokenizerFast`] (mBART-50 model)\n",
      " |          - **mega** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (MEGA model)\n",
      " |          - **megatron-bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Megatron-BERT model)\n",
      " |          - **mgp-str** -- [`MgpstrTokenizer`] (MGP-STR model)\n",
      " |          - **minimax** -- [`GPT2TokenizerFast`] (MiniMax model)\n",
      " |          - **mistral** -- [`LlamaTokenizerFast`] (Mistral model)\n",
      " |          - **mixtral** -- [`LlamaTokenizerFast`] (Mixtral model)\n",
      " |          - **mllama** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Mllama model)\n",
      " |          - **mluke** --  (mLUKE model)\n",
      " |          - **mm-grounding-dino** -- [`BertTokenizer`] or [`BertTokenizerFast`] (MM Grounding DINO model)\n",
      " |          - **mobilebert** -- [`MobileBertTokenizer`] or [`MobileBertTokenizerFast`] (MobileBERT model)\n",
      " |          - **modernbert** -- [`PreTrainedTokenizerFast`] (ModernBERT model)\n",
      " |          - **moonshine** -- [`PreTrainedTokenizerFast`] (Moonshine model)\n",
      " |          - **moshi** -- [`PreTrainedTokenizerFast`] (Moshi model)\n",
      " |          - **mpnet** -- [`MPNetTokenizer`] or [`MPNetTokenizerFast`] (MPNet model)\n",
      " |          - **mpt** -- [`GPTNeoXTokenizerFast`] (MPT model)\n",
      " |          - **mra** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (MRA model)\n",
      " |          - **mt5** -- [`MT5TokenizerFast`] (MT5 model)\n",
      " |          - **musicgen** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (MusicGen model)\n",
      " |          - **musicgen_melody** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (MusicGen Melody model)\n",
      " |          - **mvp** -- [`MvpTokenizer`] or [`MvpTokenizerFast`] (MVP model)\n",
      " |          - **myt5** -- [`MyT5Tokenizer`] (myt5 model)\n",
      " |          - **nemotron** -- [`PreTrainedTokenizerFast`] (Nemotron model)\n",
      " |          - **nezha** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Nezha model)\n",
      " |          - **nllb** -- [`NllbTokenizerFast`] (NLLB model)\n",
      " |          - **nllb-moe** -- [`NllbTokenizerFast`] (NLLB-MOE model)\n",
      " |          - **nystromformer** -- [`AlbertTokenizerFast`] (Nystr√∂mformer model)\n",
      " |          - **olmo** -- [`GPTNeoXTokenizerFast`] (OLMo model)\n",
      " |          - **olmo2** -- [`GPTNeoXTokenizerFast`] (OLMo2 model)\n",
      " |          - **olmoe** -- [`GPTNeoXTokenizerFast`] (OLMoE model)\n",
      " |          - **omdet-turbo** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OmDet-Turbo model)\n",
      " |          - **oneformer** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OneFormer model)\n",
      " |          - **openai-gpt** -- [`OpenAIGPTTokenizer`] or [`OpenAIGPTTokenizerFast`] (OpenAI GPT model)\n",
      " |          - **opt** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (OPT model)\n",
      " |          - **owlv2** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OWLv2 model)\n",
      " |          - **owlvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OWL-ViT model)\n",
      " |          - **paligemma** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (PaliGemma model)\n",
      " |          - **pegasus** -- [`PegasusTokenizerFast`] (Pegasus model)\n",
      " |          - **pegasus_x** -- [`PegasusTokenizerFast`] (PEGASUS-X model)\n",
      " |          - **perceiver** -- [`PerceiverTokenizer`] (Perceiver model)\n",
      " |          - **persimmon** -- [`LlamaTokenizerFast`] (Persimmon model)\n",
      " |          - **phi** -- [`CodeGenTokenizer`] or [`CodeGenTokenizerFast`] (Phi model)\n",
      " |          - **phi3** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Phi3 model)\n",
      " |          - **phimoe** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (Phimoe model)\n",
      " |          - **phobert** -- [`PhobertTokenizer`] (PhoBERT model)\n",
      " |          - **pix2struct** -- [`T5Tokenizer`] or [`T5TokenizerFast`] (Pix2Struct model)\n",
      " |          - **pixtral** -- [`PreTrainedTokenizerFast`] (Pixtral model)\n",
      " |          - **plbart** --  (PLBart model)\n",
      " |          - **prophetnet** -- [`ProphetNetTokenizer`] (ProphetNet model)\n",
      " |          - **qdqbert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (QDQBert model)\n",
      " |          - **qwen2** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2 model)\n",
      " |          - **qwen2_5_omni** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2_5Omni model)\n",
      " |          - **qwen2_5_vl** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2_5_VL model)\n",
      " |          - **qwen2_audio** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2Audio model)\n",
      " |          - **qwen2_moe** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2MoE model)\n",
      " |          - **qwen2_vl** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen2VL model)\n",
      " |          - **qwen3** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen3 model)\n",
      " |          - **qwen3_moe** -- [`Qwen2Tokenizer`] or [`Qwen2TokenizerFast`] (Qwen3MoE model)\n",
      " |          - **rag** -- [`RagTokenizer`] (RAG model)\n",
      " |          - **realm** -- [`RealmTokenizer`] or [`RealmTokenizerFast`] (REALM model)\n",
      " |          - **recurrent_gemma** -- [`GemmaTokenizerFast`] (RecurrentGemma model)\n",
      " |          - **reformer** -- [`ReformerTokenizerFast`] (Reformer model)\n",
      " |          - **rembert** -- [`RemBertTokenizerFast`] (RemBERT model)\n",
      " |          - **retribert** -- [`RetriBertTokenizer`] or [`RetriBertTokenizerFast`] (RetriBERT model)\n",
      " |          - **roberta** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa model)\n",
      " |          - **roberta-prelayernorm** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa-PreLayerNorm model)\n",
      " |          - **roc_bert** -- [`RoCBertTokenizer`] (RoCBert model)\n",
      " |          - **roformer** -- [`RoFormerTokenizer`] or [`RoFormerTokenizerFast`] (RoFormer model)\n",
      " |          - **rwkv** -- [`GPTNeoXTokenizerFast`] (RWKV model)\n",
      " |          - **seamless_m4t** -- [`SeamlessM4TTokenizerFast`] (SeamlessM4T model)\n",
      " |          - **seamless_m4t_v2** -- [`SeamlessM4TTokenizerFast`] (SeamlessM4Tv2 model)\n",
      " |          - **shieldgemma2** -- [`GemmaTokenizerFast`] (Shieldgemma2 model)\n",
      " |          - **siglip** --  (SigLIP model)\n",
      " |          - **siglip2** -- [`GemmaTokenizerFast`] (SigLIP2 model)\n",
      " |          - **smollm3** -- [`PreTrainedTokenizerFast`] (SmolLM3 model)\n",
      " |          - **speech_to_text** --  (Speech2Text model)\n",
      " |          - **speech_to_text_2** -- [`Speech2Text2Tokenizer`] (Speech2Text2 model)\n",
      " |          - **speecht5** --  (SpeechT5 model)\n",
      " |          - **splinter** -- [`SplinterTokenizer`] or [`SplinterTokenizerFast`] (Splinter model)\n",
      " |          - **squeezebert** -- [`SqueezeBertTokenizer`] or [`SqueezeBertTokenizerFast`] (SqueezeBERT model)\n",
      " |          - **stablelm** -- [`GPTNeoXTokenizerFast`] (StableLm model)\n",
      " |          - **starcoder2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (Starcoder2 model)\n",
      " |          - **switch_transformers** -- [`T5TokenizerFast`] (SwitchTransformers model)\n",
      " |          - **t5** -- [`T5TokenizerFast`] (T5 model)\n",
      " |          - **t5gemma** -- [`GemmaTokenizerFast`] (T5Gemma model)\n",
      " |          - **tapas** -- [`TapasTokenizer`] (TAPAS model)\n",
      " |          - **tapex** -- [`TapexTokenizer`] (TAPEX model)\n",
      " |          - **transfo-xl** -- [`TransfoXLTokenizer`] (Transformer-XL model)\n",
      " |          - **tvp** -- [`BertTokenizer`] or [`BertTokenizerFast`] (TVP model)\n",
      " |          - **udop** -- [`UdopTokenizerFast`] (UDOP model)\n",
      " |          - **umt5** -- [`T5TokenizerFast`] (UMT5 model)\n",
      " |          - **video_llava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (VideoLlava model)\n",
      " |          - **vilt** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ViLT model)\n",
      " |          - **vipllava** -- [`LlamaTokenizer`] or [`LlamaTokenizerFast`] (VipLlava model)\n",
      " |          - **visual_bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (VisualBERT model)\n",
      " |          - **vits** -- [`VitsTokenizer`] (VITS model)\n",
      " |          - **voxtral** -- [`LlamaTokenizerFast`] (Voxtral model)\n",
      " |          - **wav2vec2** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2 model)\n",
      " |          - **wav2vec2-bert** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2-BERT model)\n",
      " |          - **wav2vec2-conformer** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2-Conformer model)\n",
      " |          - **wav2vec2_phoneme** -- [`Wav2Vec2PhonemeCTCTokenizer`] (Wav2Vec2Phoneme model)\n",
      " |          - **whisper** -- [`WhisperTokenizer`] or [`WhisperTokenizerFast`] (Whisper model)\n",
      " |          - **xclip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (X-CLIP model)\n",
      " |          - **xglm** -- [`XGLMTokenizerFast`] (XGLM model)\n",
      " |          - **xlm** -- [`XLMTokenizer`] (XLM model)\n",
      " |          - **xlm-prophetnet** --  (XLM-ProphetNet model)\n",
      " |          - **xlm-roberta** -- [`XLMRobertaTokenizerFast`] (XLM-RoBERTa model)\n",
      " |          - **xlm-roberta-xl** -- [`XLMRobertaTokenizerFast`] (XLM-RoBERTa-XL model)\n",
      " |          - **xlnet** -- [`XLNetTokenizerFast`] (XLNet model)\n",
      " |          - **xlstm** -- [`GPTNeoXTokenizerFast`] (xLSTM model)\n",
      " |          - **xmod** -- [`XLMRobertaTokenizerFast`] (X-MOD model)\n",
      " |          - **yoso** -- [`AlbertTokenizerFast`] (YOSO model)\n",
      " |          - **zamba** -- [`LlamaTokenizerFast`] (Zamba model)\n",
      " |          - **zamba2** -- [`LlamaTokenizerFast`] (Zamba2 model)\n",
      " |\n",
      " |      Params:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              Can be either:\n",
      " |\n",
      " |                  - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                  - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                    using the [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
      " |                  - A path or url to a single saved vocabulary file if and only if the tokenizer only requires a\n",
      " |                    single vocabulary file (like Bert or XLNet), e.g.: `./my_model_directory/vocab.txt`. (Not\n",
      " |                    applicable to all derived classes)\n",
      " |          inputs (additional positional arguments, *optional*):\n",
      " |              Will be passed along to the Tokenizer `__init__()` method.\n",
      " |          config ([`PretrainedConfig`], *optional*)\n",
      " |              The configuration object used to determine the tokenizer class to instantiate.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download the model weights and configuration files and override the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download:\n",
      " |              Deprecated and ignored. All downloads are now resumed by default when possible.\n",
      " |              Will be removed in v5 of Transformers.\n",
      " |          proxies (`dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          use_fast (`bool`, *optional*, defaults to `True`):\n",
      " |              Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\n",
      " |              a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\n",
      " |              is returned instead.\n",
      " |          tokenizer_type (`str`, *optional*):\n",
      " |              Tokenizer type to be loaded.\n",
      " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
      " |              should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      " |              execute code present on the Hub on your local machine.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the Tokenizer `__init__()` method. Can be used to set special tokens like\n",
      " |              `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      " |              `additional_special_tokens`. See parameters in the `__init__()` for more details.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |\n",
      " |      >>> # Download vocabulary from huggingface.co and cache.\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      " |\n",
      " |      >>> # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      " |\n",
      " |      >>> # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      " |      >>> # tokenizer = AutoTokenizer.from_pretrained(\"./test/bert_saved_model/\")\n",
      " |\n",
      " |      >>> # Download vocabulary from huggingface.co and define model-specific arguments\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False)\n",
      " |      Register a new tokenizer in this mapping.\n",
      " |\n",
      " |\n",
      " |      Args:\n",
      " |          config_class ([`PretrainedConfig`]):\n",
      " |              The configuration corresponding to the model to register.\n",
      " |          slow_tokenizer_class ([`PretrainedTokenizer`], *optional*):\n",
      " |              The slow tokenizer to register.\n",
      " |          fast_tokenizer_class ([`PretrainedTokenizerFast`], *optional*):\n",
      " |              The fast tokenizer to register.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(AutoTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50774697-ba18-446a-af99-25d3d9532309",
   "metadata": {},
   "source": [
    "## All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "611e0a76-1482-41bd-8279-944c4bea9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ All together Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8241c8ad-72b5-42c0-8428-c41c7d23ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today is going to be a good day.', 'I hope everything works out in my favor!!']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Today is going to be a good day.\"\n",
    "sentence2 = \"I hope everything works out in my favor!!\"\n",
    "\n",
    "## Tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "message_batch = [sentence1, sentence2]\n",
    "print(message_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed3f920-0690-49ab-b9ea-83f3e4e9328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2651, 2003, 2183, 2000, 2022, 1037, 2204, 2154, 1012,  102,    0],\n",
      "        [ 101, 1045, 3246, 2673, 2573, 2041, 1999, 2026, 5684,  999,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      " SequenceClassifierOutput(loss=None, logits=tensor([[-4.2881,  4.7028],\n",
      "        [-4.0785,  4.4558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "Prediction: \n",
      " tensor([[1.2453e-04, 9.9988e-01],\n",
      "        [1.9658e-04, 9.9980e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      " {0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "## In one step tokenization to classification\n",
    "import torch\n",
    "\n",
    "batch_id = tokenizer(message_batch, padding=True,  return_tensors='pt') \n",
    "print(batch_id)\n",
    "\n",
    "# out = classifier(batch_id.input_ids) # ALSD WORKS\n",
    "out = classifier(**batch_id)\n",
    "print('\\n',out)\n",
    "\n",
    "print('\\nPrediction: \\n',torch.nn.functional.softmax(out.logits, dim=-1))\n",
    "print('\\n',classifier.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ecb85a-7286-4bea-9d93-3acbfd739c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "12\n",
      "sentence 1 tokens: \n",
      " tensor([ 101, 2651, 2003, 2183, 2000, 2022, 1037, 2204, 2154, 1012,  102])\n",
      "\n",
      "sentence 2 tokens: \n",
      " tensor([ 101, 1045, 3246, 2673, 2573, 2041, 1999, 2026, 5684,  999,  999,  102])\n",
      "12\n",
      "[101, 2651, 2003, 2183, 2000, 2022, 1037, 2204, 2154, 1012, 102, 0] \n",
      "\n",
      "\n",
      "Tokenized_batch: \n",
      " [[101, 2651, 2003, 2183, 2000, 2022, 1037, 2204, 2154, 1012, 102, 0], [101, 1045, 3246, 2673, 2573, 2041, 1999, 2026, 5684, 999, 999, 102]]\n",
      "\n",
      "Tensor_batch: \n",
      " tensor([[ 101, 2651, 2003, 2183, 2000, 2022, 1037, 2204, 2154, 1012,  102,    0],\n",
      "        [ 101, 1045, 3246, 2673, 2573, 2041, 1999, 2026, 5684,  999,  999,  102]])\n",
      "\n",
      "Logits on batch: \n",
      " SequenceClassifierOutput(loss=None, logits=tensor([[-4.2720,  4.6921],\n",
      "        [-4.0785,  4.4558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "sentence 1 logits \n",
      " tensor([[-4.2720,  4.6921]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "sentence 2 logits \n",
      " tensor([[-4.0785,  4.4558]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Batched sentence 1 & 2 logite \n",
      " SequenceClassifierOutput(loss=None, logits=tensor([[-4.2881,  4.7028],\n",
      "        [-4.0785,  4.4558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "Predictions:  tensor([[1.2790e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In multiple step tokenization to classification\n",
    "\n",
    "print(len(tokenizer(sentence1).input_ids))\n",
    "print(len(tokenizer(sentence2).input_ids))\n",
    "\n",
    "print('sentence 1 tokens: \\n',torch.tensor(tokenizer(sentence1).input_ids))\n",
    "print('\\nsentence 2 tokens: \\n',torch.tensor(tokenizer(sentence2).input_ids))\n",
    "\n",
    "tokenized_sentence1 = tokenizer(sentence1).input_ids\n",
    "tokenized_sentence2 = tokenizer(sentence2).input_ids\n",
    "\n",
    "tokenized_sentence1.append(tokenizer.pad_token_id)\n",
    "print(len(tokenized_sentence1))\n",
    "print(tokenized_sentence1,'\\n')\n",
    "\n",
    "\n",
    "batched = [tokenized_sentence1, tokenized_sentence2]\n",
    "print('\\nTokenized_batch: \\n',batched)\n",
    "print('\\nTensor_batch: \\n',torch.tensor(batched))\n",
    "\n",
    "\n",
    "# Prediction\n",
    "# print('Logits on sentence 1: ', classifier())\n",
    "print('\\nLogits on batch: \\n',classifier(torch.tensor(batched)))\n",
    "\n",
    "attn_mask = [[1,1,1,1,1,1,1,1,1,1,1,0],[1,1,1,1,1,1,1,1,1,1,1,1]]\n",
    "\n",
    "print(\"\\nsentence 1 logits \\n\", classifier(torch.tensor([tokenized_sentence1])).logits)\n",
    "print('\\nsentence 2 logits \\n', classifier(torch.tensor([tokenized_sentence2])).logits)\n",
    "\n",
    "print('\\nBatched sentence 1 & 2 logite \\n', classifier(torch.tensor(batched), attention_mask=torch.tensor(attn_mask))) # CAUSED RESULT TO BE DIFFERENT!!!\n",
    "\n",
    "print('\\nPredictions: ', torch.nn.functional.softmax(classifier(torch.tensor([tokenized_sentence1])).logits, dim=-1))\n",
    "classifier.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec9bb4-a84f-43df-b6e7-efb94b007c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
