{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain (Legacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubAgents\n",
    "\n",
    "from langchain.tools import tool\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import levene\n",
    "from scipy.stats import f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def calculate_PC(data):\n",
    "    '''\n",
    "    Calculate the principal components of the input data\n",
    "    '''\n",
    "    mu = data.to_numpy().mean(axis=0)\n",
    "    std = data.to_numpy().std(axis=0)\n",
    "\n",
    "    standardized_data = (data.to_numpy()-mu)/std\n",
    "    cov_matrix = np.cov(standardized_data, rowvar=False)\n",
    "\n",
    "    eigenvalue,eigenvector = eigh(cov_matrix)\n",
    "    eigenvalue = eigenvalue[::-1]\n",
    "    eigenvector = eigenvector[:,::-1]\n",
    "\n",
    "    # Apply 1st 2 principal components to the dataset\n",
    "    transformed_data = standardized_data @ np.vstack((eigenvector[:,0], eigenvector[:,1])).T\n",
    "    \n",
    "    pc1 = transformed_data[:,0].tolist()\n",
    "    pc2 = transformed_data[:,1].tolist()\n",
    "\n",
    "    return [pc1,pc2] \n",
    "\n",
    "\n",
    "def levene_test(data):\n",
    "    ''' \n",
    "    Calculate the equality of variances for 2 or more groups | F(1-aplha, N-k)\n",
    "    \n",
    "    Args:\n",
    "    data (pd.dataframe): dataframe of groups\n",
    "    k (int): numner of different groups to which the sampled case belong\n",
    "    N (int): total number of cases in all groups\n",
    "    Ni (int): number of cases in the ith group\n",
    "    Yij (array): the value of the measured variable for the jth case from the ith group\n",
    "    Zij (float): the mean or median of the ith group\n",
    "    Zi (float): is the mean of the Zij for group i\n",
    "    Z (float): is the mean of all Zij\n",
    "\n",
    "    Return:\n",
    "    W (float): F statistic 0.05\n",
    "    pval (float): p value\n",
    "    '''\n",
    "    data = np.array(data).T\n",
    "\n",
    "    N = data.shape[0]*data.shape[1]\n",
    "    k = data.shape[1]\n",
    "    Ni = data.shape[0]\n",
    "\n",
    "    Zij = [np.abs(data.T[i]-data.T[i].mean()) for i in range(k)]\n",
    "\n",
    "    Zi = [np.divide(1,Ni)*np.sum(Zij[i]) for i in range(k)]\n",
    "\n",
    "    Z = np.divide(1,N)*np.sum([np.sum(Zij[i]) for i in range(k)])\n",
    "\n",
    "    W = np.divide((N-k),(k-1))*np.divide(np.sum(Ni*(Zi-Z)**2),np.sum([np.sum((Zij[i]-Zi[i])**2) for i in range(k)]))\n",
    "    \n",
    "    pval = 1-f.cdf(W,k-1,N-k)\n",
    "\n",
    "    return W, pval\n",
    "\n",
    "def anova_one_way(dd):\n",
    "    '''     \n",
    "    Args:\n",
    "    X (float): overall mean\n",
    "    Xj (float): mean of group j\n",
    "    n (int): sample size of group j\n",
    "    Xij = the ith observation in group j\n",
    "\n",
    "    '''\n",
    "\n",
    "    k = dd.shape[1]\n",
    "    n = dd.shape[0]\n",
    "    N = k*n\n",
    "\n",
    "    X = np.array([dd[col].mean() for col in dd.columns]).mean()\n",
    "    Xj = np.array([dd[col].mean() for col in dd.columns])\n",
    "    Xij = np.array([dd[col]for col in dd.columns])\n",
    "\n",
    "    SSR = n*np.sum((Xj-X)**2) # Regression sum of squares\n",
    "    SSE = np.sum([np.sum((Xij[i]-Xj[i])**2) for i in range(len(Xj))]) # Error sum of squares\n",
    "    SST = SSR+SSE # Total sum of squares\n",
    "\n",
    "    df1 = k-1\n",
    "    df2 = N-k\n",
    "\n",
    "    MS = SSR/df1 # Mean squares\n",
    "    MSE = SSE/df2 # Mean square error\n",
    "    F = MS/MSE # F-statistic\n",
    "    pval = 1-f.cdf(F,k-1,N-k)\n",
    "    return F, pval\n",
    "\n",
    "@tool\n",
    "def get_columns(col_input) -> list:\n",
    "    '''\n",
    "    Pull columns to perform statistics on\n",
    "    '''\n",
    "    if col_input == \"all\" or \"*\":\n",
    "       \n",
    "        columns = df.columns.to_list()\n",
    "        return columns\n",
    "\n",
    "@tool\n",
    "def covariance_tool(columns: str):\n",
    "    '''\n",
    "    Calculate the covariance of the columns of data \n",
    "\n",
    "    Args:\n",
    "    columns (str): list of columns to perform covariance calculation\n",
    "\n",
    "    Return:\n",
    "    Response with the calculated covariance matrix\n",
    "    '''\n",
    "\n",
    "    columns = [col.strip().strip('\"').strip(\"'\") for col in columns.split(',')]\n",
    "\n",
    "    \n",
    "    missing = [col for col in columns if col not in df.columns]\n",
    "    if missing:\n",
    "        return f'Columns not found: {','.join(missing)}'\n",
    "    cov_matrix = df[columns].cov()\n",
    "    return f' Covariance Matrix: \\n{cov_matrix.to_string()}'\n",
    "\n",
    "@tool\n",
    "def correlation_tool(columns: str):\n",
    "    '''\n",
    "    Calculate the correlation of the columns of data\n",
    "\n",
    "    Args:\n",
    "    columns (str): list of columns to perform correlation calculation on\n",
    "\n",
    "    Return:\n",
    "    Response with the calculated correlation matrix\n",
    "    '''\n",
    "\n",
    "    columns = [col.strip().strip('\"').strip(\"'\") for col in columns.split(',')]\n",
    "\n",
    "    missing = [col for col in columns if col not in df.columns]\n",
    "    if missing:\n",
    "        return f'Columns not found: {','.join(missing)}'\n",
    "    corr_matrix = df[columns].corr()\n",
    "    return f'Correlation Matrix: {corr_matrix.to_string()}'\n",
    "\n",
    "@tool\n",
    "def normality_tool(columns: str):\n",
    "    '''\n",
    "    Calculate if the input data is normal\n",
    "\n",
    "    Args:\n",
    "    df (pd.dataframe): dataframe containig raw data\n",
    "\n",
    "    Return:\n",
    "    Response on whether data is normal or not, explicitly stating which columns are not\n",
    "    '''\n",
    "\n",
    "    columns = [col.strip().strip('\"').strip(\"'\") for col in columns.split(',')]\n",
    "\n",
    "    missing = [col for col in columns if col not in df.columns]\n",
    "    if missing:\n",
    "        return f'Columns not found: {','.join(missing)}'\n",
    "\n",
    "    not_normal = []\n",
    "\n",
    "    for col in columns:\n",
    "        tstat, pval = shapiro(df[col])\n",
    "        if pval < 0.05: # reject H0\n",
    "            not_normal.append(col)\n",
    "            return f'Column {col} is not normally distributed, and not recommended to include in ANOVA analysis'\n",
    "    return f'Data is normal'\n",
    "\n",
    "@tool\n",
    "def levene_test_tool(columns: str):\n",
    "    ''' \n",
    "    Calculate the equality of variances for 2 or more groups | F(1-aplha, N-k)\n",
    "    '''\n",
    "\n",
    "    columns = [col.strip().strip('\"').strip(\"'\") for col in columns.split(',')]\n",
    "\n",
    "    missing = [col for col in columns if col not in df.columns]\n",
    "    if missing:\n",
    "        return f'Columns not found: {','.join(missing)}'\n",
    "    \n",
    "\n",
    "    data = df[columns]\n",
    "    \n",
    "\n",
    "    return levene_test(data)\n",
    "\n",
    "@tool    \n",
    "def calculate_ANOVA_tool(columns: str):\n",
    "    '''\n",
    "    Compare the means of 3 or more independant groups to determine statistical difference between popuplaiton means.\n",
    "    H0: The mean of the independent groups are all the same, there is no statistical difference. difference is due to random sampling from same population\n",
    "    Test for normality within ea group with shapiro-wilks test (H0: data was drawn from a normal distribution) pval < 0.05 reject H0\n",
    "    Test for homegenity of variance with levene test (H0: data has equal variance) p_val < 0.05 reject H0 at lease 1 of the groups is not heteroskedastic Anova can be used\n",
    "    H0: two or more groups have the same population mean\n",
    "    Calculate the amount of variance between groups means to the amount of variation within each group  group variation > within group variation => difference in mean not random\n",
    "    '''\n",
    "\n",
    "    columns = [col.strip().strip('\"').strip(\"'\") for col in columns.split(',')]\n",
    "\n",
    "    missing = [col for col in columns if col not in df.columns]\n",
    "    if missing:\n",
    "        return f'Columns not found: {','.join(missing)}'\n",
    "    \n",
    "\n",
    "    data = df[columns]\n",
    "    \n",
    "\n",
    "    return anova_one_way(data)\n",
    "\n",
    "@tool\n",
    "def PCA_tool(columns: str) ->list:\n",
    "    '''\n",
    "    Calculates the 1st two principal components of a dataset\n",
    "    '''\n",
    "\n",
    "    columns = [col.strip().strip('\"').strip(\"'\") for col in columns.split(',')]\n",
    "\n",
    "    \n",
    "    return calculate_PC(df[columns])\n",
    "\n",
    "@tool\n",
    "def plot_data(mylist: str):\n",
    "    '''\n",
    "    Create a scatter plot \n",
    "    '''\n",
    "    import matplotlib.pyplot as plt \n",
    "    import ast\n",
    "    \n",
    "    mylist = mylist.strip('\" \"')\n",
    "    mylist = ast.literal_eval(mylist)\n",
    "    pc1 = np.array(mylist[0])\n",
    "    pc2 = np.array(mylist[1])\n",
    "\n",
    "\n",
    "\n",
    "    plt.scatter(pc1,pc2)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    # plt.savefig(\"pca_result.png\")\n",
    "    plt.show()\n",
    "    return \"PCA plot saved to pca_result.png\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def stat_agent():\n",
    "    ''' \n",
    "    One-shot Stat Agent\n",
    "    '''\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_message=True)\n",
    "    llm = BedrockChat(model_id='anthropic.claude-3-5-sonnet-20240620-v1:0', region_name='us-east-1', model_kwargs={'max_tokens': 3000, 'temperature': 0})\n",
    "\n",
    "    sub_agent = [covariance_tool, correlation_tool, normality_tool, levene_test_tool, get_columns, calculate_ANOVA_tool, PCA_tool, plot_data]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=sub_agent,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "        agent_kwargs={'system_message': 'You are a Statistics agent '}\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4651/2534861037.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_message=True)\n",
      "/tmp/ipykernel_4651/2534861037.py:10: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import ChatBedrock``.\n",
      "  llm = BedrockChat(model_id='anthropic.claude-3-5-sonnet-20240620-v1:0', region_name='us-east-1', model_kwargs={'max_tokens': 3000, 'temperature': 0})\n",
      "/tmp/ipykernel_4651/2534861037.py:14: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer this question, we need to follow these steps:\n",
      "1. Get all the columns from the dataframe\n",
      "2. Calculate the principal components using those columns\n",
      "3. Create a scatter plot of the first two principal components\n",
      "\n",
      "Let's start with getting all the columns.\n",
      "\n",
      "Action: get_columns\n",
      "Action Input: all\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m['A0', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mGreat, we now have all the columns from the dataframe. Let's proceed with calculating the principal components using these columns.\n",
      "\n",
      "Action: PCA_tool\n",
      "Action Input: A0,A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[[-0.8279801125859164, -1.8748439208408956, -0.9639972848511245, 1.7064715093789358, -3.674450747526948, 1.5901198630406577, -0.9601324619449182, -2.3963045488865466, 0.6049511023882346, 0.20410869888476968, 0.9799686007125502, -0.9871319474789318, -0.2531322066426445, 2.9761087256826713, 3.496544317832839, -0.04073974852588082, -0.18050710373411924, 0.3476974768882603, 3.519289973481336, -3.2660401852723435], [0.17702347901541626, 2.9740327864943628, -1.314219285745275, -1.4084897210219536, -2.7943212785441816, -0.7610190055223119, 1.8326953276131588, -0.3636056422527247, 2.258025982026305, 1.435574201873288, 0.3370210206542136, 2.248175394572511, 1.3893196802712418, 0.9293976662562928, 0.2599447823337278, -1.2119095046882802, -2.1700619763951288, -2.176387888541321, -1.3924040652248222, -0.24879195317454347]]\u001b[0m\n",
      "Thought:"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "stat_agent = stat_agent()\n",
    "# df = pd.read_csv('sample_df.csv')\n",
    "data = load_wine()\n",
    "lbl = data['target_names']\n",
    "header = [a+str(n) for n,a in enumerate(np.repeat('A',data['data'].shape[1]))]\n",
    "df = pd.DataFrame(data['data'], columns=header)\n",
    "df = df.iloc[:20,:]\n",
    "\n",
    "stat_agent.invoke('Get all the columns from df and calculate the principal components then create a scatter plot of the 2 different principal components')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/\")\n",
    "import requests\n",
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "load_dotenv(\"/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/.env\")\n",
    "\n",
    "def get_openai_llms():\n",
    "    '''get a list of llms available in Azure OpenAI'''\n",
    "    response = requests.get(os.getenv('AZURE_OPENAI_BASEURL'))\n",
    "    returned_payload = response.json()\n",
    "\n",
    "    return [models for models in returned_payload['nonprod'].keys()]\n",
    "\n",
    "\n",
    "def get_aws_llms():\n",
    "    '''get a list of llms available in AWS Bedrock'''\n",
    "    bedrock_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "    response = bedrock_client.list_foundation_models()\n",
    "\n",
    "    llm_dict = {}\n",
    "    for obj in  response['modelSummaries']:\n",
    "        llm_dict[obj['modelName']] = obj['modelId']\n",
    "\n",
    "    return llm_dict\n",
    "\n",
    "def aws_llm(modelid: str):  \n",
    "    '''Create LangChain Object for AWS Bedrock llm'''     \n",
    "\n",
    "    return ChatBedrockConverse(model_id=modelid, region_name='us-east-1')\n",
    "\n",
    "def openai_llm(modle_id: str):\n",
    "    '''Create LangChain LLM object for Azure OpenAI llm'''\n",
    "    api_response = requests.get(os.getenv('AZURE_OPENAI_BASEURL'))\n",
    "    payload = api_response.json()\n",
    "\n",
    "    gllm = AzureChatOpenAI(\n",
    "        deployment_name=modle_id,  \n",
    "        openai_api_version=\"2024-12-01-preview\",\n",
    "        azure_endpoint=payload['nonprod'][modle_id][0]['endpoint'],\n",
    "        api_key=os.getenv('AZURE_OPENAI_KEY'),\n",
    "        temperature=0)\n",
    "    return gllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dall-e-3',\n",
       " 'gpt-35-turbo',\n",
       " 'gpt-35-turbo-instruct',\n",
       " 'gpt-4-turbo',\n",
       " 'gpt-4.1',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-nano',\n",
       " 'gpt-4.5',\n",
       " 'gpt-4o-audio',\n",
       " 'gpt-4o-global',\n",
       " 'gpt-4o-mini-global',\n",
       " 'gpt-4o-regional',\n",
       " 'gpt-5',\n",
       " 'gpt-5-chat',\n",
       " 'gpt-5-mini',\n",
       " 'gpt-5-nano',\n",
       " 'gpt-image-1',\n",
       " 'model-router',\n",
       " 'o1',\n",
       " 'o1-mini',\n",
       " 'o3',\n",
       " 'o3-mini',\n",
       " 'o4-mini',\n",
       " 'text-embedding-3-large',\n",
       " 'text-embedding-3-small',\n",
       " 'text-embedding-ada-002']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_openai_llms() #== 'dall-e-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_agent, AgentType\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BedrockChat\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstat_agent\u001b[39m():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def stat_agent():\n",
    "    ''' \n",
    "    One-shot Stat Agent\n",
    "    '''\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_message=True)\n",
    "    # llm = openai_llm(get_openai_llms()[4])\n",
    "    llm = openai_llm('gpt-4.1')\n",
    "\n",
    "    sub_agent = [covariance_tool, correlation_tool, normality_tool, levene_test_tool, get_columns, calculate_ANOVA_tool, PCA_tool, plot_data]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=sub_agent,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "        agent_kwargs={'system_message': 'You are a Statistics agent'}\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "stat_agent = stat_agent()\n",
    "# df = pd.read_csv('sample_df.csv')\n",
    "data = load_wine()\n",
    "lbl = data['target_names']\n",
    "header = [a+str(n) for n,a in enumerate(np.repeat('A',data['data'].shape[1]))]\n",
    "df = pd.DataFrame(data['data'], columns=header)\n",
    "df = df.iloc[:20,:]\n",
    "\n",
    "stat_agent.invoke('Get all the columns from df and calculate the principal components then create a scatter plot of the 2 different principal components')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
