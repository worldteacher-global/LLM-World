{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://github.com/openai/gpt-oss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 225197.53it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:06<00:00, 22.33s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', # Tells the pipeline we would like to generate text\n",
    "    model='openai/gpt-oss-20b', # specifies the model\n",
    "    torch_dtype='auto', # data type the model uses in PyTorch\n",
    "    device_map='auto') # maps the model to a device\n",
    "\n",
    "messages = [{'role':'user', 'content':'Explain what mixture-of-experts is'}]\n",
    "\n",
    "out = pipe(messages,\n",
    " max_new_tokens=3000, # number of tokens the model will generate (output length)\n",
    " temperature=1.0) # 1 = baseline randomness in token selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'Explain what mixture-of-experts is'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['generated_text'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Explain what mixture-of-experts is'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['role', 'content'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Explain what mixture-of-experts is\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][0]['role'])\n",
    "print(out[0]['generated_text'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind's Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\n",
      "\n",
      "We'll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\n",
      "\n",
      "Given chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\n",
      "\n",
      "Let's compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\n",
      "\n",
      "At heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \n",
      "During training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\n",
      "\n",
      "* The combined capacity is far larger than any single expert.\n",
      "* Only a handful of experts are active for a given example—keeping compute cheap.\n",
      "* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\n",
      "\n",
      "### 1. Historical roots\n",
      "\n",
      "| Year | Paper | Core idea |\n",
      "|------|-------|------------|\n",
      "| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\n",
      "| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\n",
      "| 2017 | *“Switching Layers”* (Shazeer et al.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\n",
      "| 2020 | *“Switch Transformer”* + *“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\n",
      "\n",
      "### 2. Architecture in a nutshell\n",
      "\n",
      "```\n",
      "x  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\n",
      "                                              │      │      │        │ \n",
      "                                           out1   out2   …     oute\n",
      "                                              │      │      │        │ \n",
      "                                             Σ (gatei · outi) ──►  yhat\n",
      "```\n",
      "\n",
      "* **Input `x`** (text, image, etc.) goes into the *gate*.\n",
      "* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \n",
      "  * *Softmax* gate (dense) – every expert contributes a little.  \n",
      "  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\n",
      "* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\n",
      "* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\n",
      "\n",
      "#### Why sparsity?\n",
      "\n",
      "* **Compute:** Only `k ≪ E` experts are evaluated per token → ~k/E of the full computation.\n",
      "* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\n",
      "* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\n",
      "\n",
      "### 3. Training an MoE\n",
      "\n",
      "1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\n",
      "2. **Regularization on the gate** to prevent collapse into a single expert:\n",
      "   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\n",
      "   * **Entropy loss**: keep the distribution flat if you want diverse routing.\n",
      "   * **Auxiliary loss**: guide experts to output different representations.\n",
      "3. **Drop‑out or noise** can be applied to the gate to improve exploration.\n",
      "\n",
      "In practice, the training pipeline alternates between:\n",
      "  * Forward pass: compute `g`, select experts, evaluate, pool.\n",
      "  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\n",
      "\n",
      "### 4. Common Variants\n",
      "\n",
      "| Variant | Gate | Activation | Use‑case |\n",
      "|---------|------|------------|----------|\n",
      "| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\n",
      "| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\n",
      "| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1 × 10¹³ param BERT‑style model |\n",
      "| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\n",
      "| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\n",
      "| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\n",
      "\n",
      "### 5. Practical Applications\n",
      "\n",
      "| Domain | How MoE Helps | Example Model |\n",
      "|--------|--------------|---------------|\n",
      "| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\n",
      "| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\n",
      "| Speech | Accented / noisy variants | MoE conformer for ASR |\n",
      "| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\n",
      "| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\n",
      "| Reinforcement Learning | Policy specialization for states | MoE policy networks |\n",
      "\n",
      "### 6. Why is MoE attractive now?\n",
      "\n",
      "| Feature | Why it matters |\n",
      "|---------|----------------|\n",
      "| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\n",
      "| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\n",
      "| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\n",
      "| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\n",
      "| **Sparse routing** | Enables training very large models on commodity hardware. |\n",
      "\n",
      "### 7. Challenges & Open Issues\n",
      "\n",
      "| Issue | Impact | Mitigation |\n",
      "|-------|---------|------------|\n",
      "| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\n",
      "| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\n",
      "| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\n",
      "| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\n",
      "| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\n",
      "\n",
      "### 8. Quick pseudo‑code (PyTorch)\n",
      "\n",
      "```python\n",
      "class MoE(nn.Module):\n",
      "    def __init__(self, num_experts, hidden_dim, top_k=2):\n",
      "        super().__init__()\n",
      "        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\n",
      "        self.experts = nn.ModuleList(\n",
      "            [nn.Sequential(\n",
      "                nn.Linear(hidden_dim, hidden_dim),\n",
      "                nn.ReLU(),\n",
      "                nn.Linear(hidden_dim, hidden_dim))\n",
      "             for _ in range(num_experts)] )\n",
      "        self.top_k = top_k\n",
      "\n",
      "    def forward(self, x):\n",
      "        # x: (batch, hidden_dim)\n",
      "        gate_logits = self.gate(x)                     # (batch, E)\n",
      "        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\n",
      "\n",
      "        # sparse top‑k selection\n",
      "        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\n",
      "        sparse_gate         = torch.zeros_like(gate_logits).scatter(\n",
      "            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\n",
      "\n",
      "        # expert outputs (only top‑k evaluated)\n",
      "        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\n",
      "        # expert_outs: (batch, E, hidden_dim)\n",
      "\n",
      "        # weighted sum\n",
      "        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\n",
      "        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\n",
      "        return out\n",
      "```\n",
      "\n",
      "*In practice* you’d add:\n",
      "* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\n",
      "* **Switch entropy**: `-gate_probs.log().mean()`\n",
      "\n",
      "### 9. Take‑away\n",
      "\n",
      "* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\n",
      "* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\n",
      "* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\n",
      "* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\n",
      "\n",
      "Feel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][1]['role'])\n",
    "print(out[0]['generated_text'][1]['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced inference .generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 208671.84it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:43<00:00, 34.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-15\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Explain what MXFP4 quantization is.<|end|><|start|>assistant<|channel|>analysis<|message|>The user asks: \"Explain what MXFP4 quantization is.\" They likely refer to some technology, maybe in audio or signal processing? \"MXFP4\" seems unfamiliar. Could be a specific standard or algorithm. Might refer to \"Mixed-Precision Fixed-Point 4\" or \"Maximum eXponential Fixed Precision 4\"? Or \"Matrix Factorization Pair 4\"? Or maybe \"Milton's eXperimental Fixed Pattern 4\"? I need to interpret. Search memory: \"MXFP4\" appears in the context of \"MXFP4 quantization\" maybe in deep learning or neural networks? There's \"MXFP\" stands for \"Mixed-Precision Floating Point\"? Actually \"MXFP\" could be \"Microsoft eXpress Forward Propagation\"? Not sure.\n",
      "\n",
      "Let's think: \"MXFP4\" might be a quantization scheme used by \"MXFP\" from Mathematically eXtracted Free Parameter 4?? Might be a mis-typed \"MX\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/gpt-oss-20b\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    " \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    " \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    " \n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    " \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual-Thinking Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Rreasoning dataset where the chain-of-thought has been translated into several languages such as French, Spanish, and German. By fine-tuning openai/gpt-oss-20b on this dataset, it will learn to generate reasoning steps in these languages, and thus its reasoning process can be interpreted by users who speak those languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmony Response Format\n",
    "\n",
    "# developer: The developer message is used to provide custom instructions for the model (what we usually call the system role).\n",
    "\n",
    "# user: The user message is used to provide the input to the model.\n",
    "\n",
    "# assistant: Output by the model which can either be a tool call or a message output. The output might also be associated with a particular “channel” identifying what the intent of the message is.\n",
    "\n",
    "# analysis: These are messages that are being used by the model for its chain-of-thought\n",
    "\n",
    "# final: Messages tagged in the final channel are messages intended to be shown to the end-user and represent the responses from the model.\n",
    "\n",
    "# messages: The list of messages that combine the content of the above to produce a full conversation. This is the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692f785b5b9c4f0abfb194e92cbbe277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dad0734c0994675a42cc637daf9bd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/5.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9026d518ac4ffcae2e643c1e33f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'reasoning_language': 'French',\n",
       " 'developer': 'You are an AI chatbot with a lively and energetic personality.',\n",
       " 'user': 'Can you show me the latest trends on Twitter right now?',\n",
       " 'analysis': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\",\n",
       " 'final': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       " 'messages': [{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.',\n",
       "   'role': 'system',\n",
       "   'thinking': None},\n",
       "  {'content': 'Can you show me the latest trends on Twitter right now?',\n",
       "   'role': 'user',\n",
       "   'thinking': None},\n",
       "  {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       "   'role': 'assistant',\n",
       "   'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('HuggingFaceH4/Multilingual-Thinking', split='train')\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to fine-tune the model, we need to convert these messages into a format that the model can understand. In practice this is done by formatting each message with the model's chat template and then tokenizing the resulting text.\n",
    "\n",
    "# *** TRL library does this automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual input formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8874bb35ab7439ea9cb1f059d82b393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33567c063f30425b9453342b3cde21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d5c48f967b4e71924cf5e83660a7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706e0cde206444b3bf91d0273b5a90e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai/gpt-oss-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.', 'role': 'system', 'thinking': None}, {'content': 'Can you show me the latest trends on Twitter right now?', 'role': 'user', 'thinking': None}, {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!', 'role': 'assistant', 'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['messages']\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-16\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: French\n",
      "\n",
      "You are an AI chatbot with a lively and energetic personality.\n",
      "\n",
      "<|end|><|start|>user<|message|>Can you show me the latest trends on Twitter right now?<|end|><|start|>assistant<|channel|>analysis<|message|>D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\n",
      "\n",
      "Je devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.<|end|><|start|>assistant<|channel|>final<|message|>Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\n",
      "\n",
      "1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \n",
      "2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \n",
      "3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \n",
      "4. **Check regional trends** – they often differ by location!  \n",
      "\n",
      "Remember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \n",
      "\n",
      "Want me to brainstorm *what* might trend next? I’ve got ideas!<|return|>\n"
     ]
    }
   ],
   "source": [
    "conversation = tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190dac190e07490bbfb2962717be4a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0262d04d4a3840708cbeef14ce4b3923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Download the weights\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
    "\n",
    "\n",
    "quantization_config = Mxfp4Config(dequantize=True) # Loads the necessary config for training. Allows mixed precision training with a special 4-bit floating point format (MXFP4)\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'attn_implementation':'eager',\n",
    "#     'torch_dtype':torch.bfloat16,\n",
    "#     'quantization_config'=quantization_config,\n",
    "#     'use_cache':False,\n",
    "#     'device_map':'auto'\n",
    "#     }\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation='eager', # better performance\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    use_cache=False, # using gradient checkpointing (slectivley saving/recomputing intermediate activations during backpropagation instead of storing them all)\n",
    "    device_map='auto') \n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('openai/gpt-oss-20b',**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-17\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>¿Cuál es el capital de Australia?<|end|><|start|>assistant<|channel|>analysis<|message|>We need to answer in Spanish presumably. The question: \"¿Cuál es el capital de Australia?\" We answer: Canberra. Provide short answer.<|end|><|start|>assistant<|channel|>final<|message|>La capital de Australia es Canberra.<|return|>\n"
     ]
    }
   ],
   "source": [
    "# tokenize sample prompt\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "# Test untrained model on sample prompt\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use LoRA (Low-rank Adaptation) to fine-tune the model (fine-tune specific layers of the model)  \n",
    "  \n",
    "## Parameter Effiecient Fine-Tuning\n",
    "# Wrap model as PeftModel and define LoRA config\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                   # enable more trainable parameters\n",
    "    lora_alpha=16,\n",
    "    target_modules='all-linear', # layers to target ie: attn\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,040,512 || all params: 20,929,797,696 || trainable%: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model,peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tuning with Transformers Renforcment Learning (TRL)\n",
    "\n",
    "## Define hyperparameters\n",
    "\n",
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    learning_rate=2e-4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4, # accumulate gradients over multiple batches before weight update \n",
    "    max_length=2048,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\":0.1},\n",
    "    output_dir=\"gpt-oss-20b-multilingual-reasoner\",\n",
    "    report_to=\"trackio\",\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-68a1abb1-3ee1703b7d902f4f0bfa8c86;74392627-baaa-47c7-98f7-073a59835f31)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m## Define the trainer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:544\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m._total_train_tokens = \u001b[32m0\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;66;03m# Initialize the Trainer. Parent class will handle:\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# - DeepSpeed configuration (through create_accelerator_and_postprocess)\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# - FSDP setup\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# - Distributed training setup\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[38;5;66;03m# - Optimizer and scheduler creation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# Initialize activation offloading context\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.activation_offloading:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/trainer.py:699\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28mself\u001b[39m.hub_model_id = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_hf_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m    701\u001b[39m     os.makedirs(\u001b[38;5;28mself\u001b[39m.args.output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/trainer.py:4725\u001b[39m, in \u001b[36mTrainer.init_hf_repo\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   4722\u001b[39m     repo_name = \u001b[38;5;28mself\u001b[39m.args.hub_model_id\n\u001b[32m   4724\u001b[39m token = token \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.hub_token\n\u001b[32m-> \u001b[39m\u001b[32m4725\u001b[39m repo_url = \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4726\u001b[39m \u001b[38;5;28mself\u001b[39m.hub_model_id = repo_url.repo_id\n\u001b[32m   4727\u001b[39m \u001b[38;5;28mself\u001b[39m.push_in_progress = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3755\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3752\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3754\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3755\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3756\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3757\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3758\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-68a1abb1-3ee1703b7d902f4f0bfa8c86;74392627-baaa-47c7-98f7-073a59835f31)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "## Define the trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
