{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://github.com/openai/gpt-oss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Inference with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 225197.53it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:06<00:00, 22.33s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', # Tells the pipeline we would like to generate text\n",
    "    model='openai/gpt-oss-20b', # specifies the model\n",
    "    torch_dtype='auto', # data type the model uses in PyTorch\n",
    "    device_map='auto') # maps the model to a device\n",
    "\n",
    "messages = [{'role':'user', 'content':'Explain what mixture-of-experts is'}]\n",
    "\n",
    "out = pipe(messages,\n",
    " max_new_tokens=3000, # number of tokens the model will generate (output length)\n",
    " temperature=1.0) # 1 = baseline randomness in token selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'Explain what mixture-of-experts is'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['generated_text'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Explain what mixture-of-experts is'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['role', 'content'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Explain what mixture-of-experts is\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][0]['role'])\n",
    "print(out[0]['generated_text'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind's Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\n",
      "\n",
      "We'll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\n",
      "\n",
      "Given chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\n",
      "\n",
      "Let's compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\n",
      "\n",
      "At heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \n",
      "During training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\n",
      "\n",
      "* The combined capacity is far larger than any single expert.\n",
      "* Only a handful of experts are active for a given example—keeping compute cheap.\n",
      "* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\n",
      "\n",
      "### 1. Historical roots\n",
      "\n",
      "| Year | Paper | Core idea |\n",
      "|------|-------|------------|\n",
      "| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\n",
      "| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\n",
      "| 2017 | *“Switching Layers”* (Shazeer et al.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\n",
      "| 2020 | *“Switch Transformer”* + *“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\n",
      "\n",
      "### 2. Architecture in a nutshell\n",
      "\n",
      "```\n",
      "x  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\n",
      "                                              │      │      │        │ \n",
      "                                           out1   out2   …     oute\n",
      "                                              │      │      │        │ \n",
      "                                             Σ (gatei · outi) ──►  yhat\n",
      "```\n",
      "\n",
      "* **Input `x`** (text, image, etc.) goes into the *gate*.\n",
      "* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \n",
      "  * *Softmax* gate (dense) – every expert contributes a little.  \n",
      "  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\n",
      "* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\n",
      "* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\n",
      "\n",
      "#### Why sparsity?\n",
      "\n",
      "* **Compute:** Only `k ≪ E` experts are evaluated per token → ~k/E of the full computation.\n",
      "* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\n",
      "* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\n",
      "\n",
      "### 3. Training an MoE\n",
      "\n",
      "1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\n",
      "2. **Regularization on the gate** to prevent collapse into a single expert:\n",
      "   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\n",
      "   * **Entropy loss**: keep the distribution flat if you want diverse routing.\n",
      "   * **Auxiliary loss**: guide experts to output different representations.\n",
      "3. **Drop‑out or noise** can be applied to the gate to improve exploration.\n",
      "\n",
      "In practice, the training pipeline alternates between:\n",
      "  * Forward pass: compute `g`, select experts, evaluate, pool.\n",
      "  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\n",
      "\n",
      "### 4. Common Variants\n",
      "\n",
      "| Variant | Gate | Activation | Use‑case |\n",
      "|---------|------|------------|----------|\n",
      "| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\n",
      "| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\n",
      "| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1 × 10¹³ param BERT‑style model |\n",
      "| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\n",
      "| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\n",
      "| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\n",
      "\n",
      "### 5. Practical Applications\n",
      "\n",
      "| Domain | How MoE Helps | Example Model |\n",
      "|--------|--------------|---------------|\n",
      "| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\n",
      "| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\n",
      "| Speech | Accented / noisy variants | MoE conformer for ASR |\n",
      "| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\n",
      "| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\n",
      "| Reinforcement Learning | Policy specialization for states | MoE policy networks |\n",
      "\n",
      "### 6. Why is MoE attractive now?\n",
      "\n",
      "| Feature | Why it matters |\n",
      "|---------|----------------|\n",
      "| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\n",
      "| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\n",
      "| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\n",
      "| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\n",
      "| **Sparse routing** | Enables training very large models on commodity hardware. |\n",
      "\n",
      "### 7. Challenges & Open Issues\n",
      "\n",
      "| Issue | Impact | Mitigation |\n",
      "|-------|---------|------------|\n",
      "| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\n",
      "| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\n",
      "| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\n",
      "| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\n",
      "| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\n",
      "\n",
      "### 8. Quick pseudo‑code (PyTorch)\n",
      "\n",
      "```python\n",
      "class MoE(nn.Module):\n",
      "    def __init__(self, num_experts, hidden_dim, top_k=2):\n",
      "        super().__init__()\n",
      "        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\n",
      "        self.experts = nn.ModuleList(\n",
      "            [nn.Sequential(\n",
      "                nn.Linear(hidden_dim, hidden_dim),\n",
      "                nn.ReLU(),\n",
      "                nn.Linear(hidden_dim, hidden_dim))\n",
      "             for _ in range(num_experts)] )\n",
      "        self.top_k = top_k\n",
      "\n",
      "    def forward(self, x):\n",
      "        # x: (batch, hidden_dim)\n",
      "        gate_logits = self.gate(x)                     # (batch, E)\n",
      "        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\n",
      "\n",
      "        # sparse top‑k selection\n",
      "        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\n",
      "        sparse_gate         = torch.zeros_like(gate_logits).scatter(\n",
      "            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\n",
      "\n",
      "        # expert outputs (only top‑k evaluated)\n",
      "        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\n",
      "        # expert_outs: (batch, E, hidden_dim)\n",
      "\n",
      "        # weighted sum\n",
      "        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\n",
      "        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\n",
      "        return out\n",
      "```\n",
      "\n",
      "*In practice* you’d add:\n",
      "* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\n",
      "* **Switch entropy**: `-gate_probs.log().mean()`\n",
      "\n",
      "### 9. Take‑away\n",
      "\n",
      "* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\n",
      "* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\n",
      "* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\n",
      "* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\n",
      "\n",
      "Feel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][1]['role'])\n",
    "print(out[0]['generated_text'][1]['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Advanced inference .generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 208671.84it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:43<00:00, 34.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-15\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Explain what MXFP4 quantization is.<|end|><|start|>assistant<|channel|>analysis<|message|>The user asks: \"Explain what MXFP4 quantization is.\" They likely refer to some technology, maybe in audio or signal processing? \"MXFP4\" seems unfamiliar. Could be a specific standard or algorithm. Might refer to \"Mixed-Precision Fixed-Point 4\" or \"Maximum eXponential Fixed Precision 4\"? Or \"Matrix Factorization Pair 4\"? Or maybe \"Milton's eXperimental Fixed Pattern 4\"? I need to interpret. Search memory: \"MXFP4\" appears in the context of \"MXFP4 quantization\" maybe in deep learning or neural networks? There's \"MXFP\" stands for \"Mixed-Precision Floating Point\"? Actually \"MXFP\" could be \"Microsoft eXpress Forward Propagation\"? Not sure.\n",
      "\n",
      "Let's think: \"MXFP4\" might be a quantization scheme used by \"MXFP\" from Mathematically eXtracted Free Parameter 4?? Might be a mis-typed \"MX\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/gpt-oss-20b\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    " \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    " \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    " \n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    " \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd73ac0678f479a8e9847e710a833f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual-Thinking Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Rreasoning dataset where the chain-of-thought has been translated into several languages such as French, Spanish, and German. By fine-tuning openai/gpt-oss-20b on this dataset, it will learn to generate reasoning steps in these languages, and thus its reasoning process can be interpreted by users who speak those languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmony Response Format\n",
    "\n",
    "# developer: The developer message is used to provide custom instructions for the model (what we usually call the system role).\n",
    "\n",
    "# user: The user message is used to provide the input to the model.\n",
    "\n",
    "# assistant: Output by the model which can either be a tool call or a message output. The output might also be associated with a particular “channel” identifying what the intent of the message is.\n",
    "\n",
    "# analysis: These are messages that are being used by the model for its chain-of-thought\n",
    "\n",
    "# final: Messages tagged in the final channel are messages intended to be shown to the end-user and represent the responses from the model.\n",
    "\n",
    "# messages: The list of messages that combine the content of the above to produce a full conversation. This is the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TRACKIO\"] = \"1\"  # disables internal tracking\n",
    "os.environ[\"TRL_GRADIO_ENABLED\"] = \"0\"      # disables Gradio UI\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"        # optional: disables WandB logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning_language': 'French',\n",
       " 'developer': 'You are an AI chatbot with a lively and energetic personality.',\n",
       " 'user': 'Can you show me the latest trends on Twitter right now?',\n",
       " 'analysis': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\",\n",
       " 'final': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       " 'messages': [{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.',\n",
       "   'role': 'system',\n",
       "   'thinking': None},\n",
       "  {'content': 'Can you show me the latest trends on Twitter right now?',\n",
       "   'role': 'user',\n",
       "   'thinking': None},\n",
       "  {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       "   'role': 'assistant',\n",
       "   'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('HuggingFaceH4/Multilingual-Thinking', split='train')\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to fine-tune the model, we need to convert these messages into a format that the model can understand. In practice this is done by formatting each message with the model's chat template and then tokenizing the resulting text.\n",
    "\n",
    "# *** TRL library does this automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual input formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8874bb35ab7439ea9cb1f059d82b393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33567c063f30425b9453342b3cde21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d5c48f967b4e71924cf5e83660a7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706e0cde206444b3bf91d0273b5a90e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai/gpt-oss-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.', 'role': 'system', 'thinking': None}, {'content': 'Can you show me the latest trends on Twitter right now?', 'role': 'user', 'thinking': None}, {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!', 'role': 'assistant', 'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['messages']\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-16\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: French\n",
      "\n",
      "You are an AI chatbot with a lively and energetic personality.\n",
      "\n",
      "<|end|><|start|>user<|message|>Can you show me the latest trends on Twitter right now?<|end|><|start|>assistant<|channel|>analysis<|message|>D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\n",
      "\n",
      "Je devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.<|end|><|start|>assistant<|channel|>final<|message|>Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\n",
      "\n",
      "1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \n",
      "2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \n",
      "3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \n",
      "4. **Check regional trends** – they often differ by location!  \n",
      "\n",
      "Remember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \n",
      "\n",
      "Want me to brainstorm *what* might trend next? I’ve got ideas!<|return|>\n"
     ]
    }
   ],
   "source": [
    "conversation = tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592b29f805cb4305990f2c25649def10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Download the weights\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Mxfp4Config, TrainingArguments\n",
    "\n",
    "\n",
    "quantization_config = Mxfp4Config(dequantize=True) # Loads the necessary config for training. Allows mixed precision training with a special 4-bit floating point format (MXFP4)\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'attn_implementation':'eager',\n",
    "#     'torch_dtype':torch.bfloat16,\n",
    "#     'quantization_config'=quantization_config,\n",
    "#     'use_cache':False,\n",
    "#     'device_map':'auto'\n",
    "#     }\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation='eager', # better performance\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    use_cache=False, # using gradient checkpointing (slectivley saving/recomputing intermediate activations during backpropagation instead of storing them all)\n",
    "    device_map='auto') \n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('openai/gpt-oss-20b',**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-17\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>¿Cuál es el capital de Australia?<|end|><|start|>assistant<|channel|>analysis<|message|>We need to answer in Spanish presumably. The question: \"¿Cuál es el capital de Australia?\" We answer: Canberra. Provide short answer.<|end|><|start|>assistant<|channel|>final<|message|>La capital de Australia es Canberra.<|return|>\n"
     ]
    }
   ],
   "source": [
    "# tokenize sample prompt\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "# Test untrained model on sample prompt\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use LoRA (Low-rank Adaptation) to fine-tune the model (fine-tune specific layers of the model)  \n",
    "  \n",
    "## Parameter Effiecient Fine-Tuning\n",
    "# Wrap model as PeftModel and define LoRA config\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                   # enable more trainable parameters\n",
    "    lora_alpha=16,\n",
    "    target_modules='all-linear', # layers to target ie: attn\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,040,512 || all params: 20,929,797,696 || trainable%: 0.0719\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model,peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tuning with Transformers Renforcment Learning (TRL)\n",
    "\n",
    "## Define hyperparameters\n",
    "\n",
    "import os\n",
    "# from trl import SFTConfig\n",
    "\n",
    "filepath = '/home/marfok/CLONED_REPOS/LLM-World/Files/OSS_Training_Output/'\n",
    "\n",
    "# training_args = SFTConfig(\n",
    "#     learning_rate=2e-4,\n",
    "#     gradient_checkpointing=True,\n",
    "#     num_train_epochs=1,\n",
    "#     logging_steps=1,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=4, # accumulate gradients over multiple batches before weight update \n",
    "#     max_length=2048,\n",
    "#     warmup_ratio=0.03,\n",
    "#     lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "#     lr_scheduler_kwargs={\"min_lr_rate\":0.1},\n",
    "#     output_dir=os.path.join(filepath,\"gpt-oss-20b-multilingual-reasoner_ipynb_\"),\n",
    "#     # report_to=\"trackio\",\n",
    "#     # push_to_hub=True\n",
    "# )\n",
    "training_args = TrainingArguments(    \n",
    "    output_dir=os.path.join(filepath,\"gpt-oss-20b-multilingual-reasoner\"),\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    fp16=True,  # enable if using GPU\n",
    "    learning_rate=2e-4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4, # accumulate gradients over multiple batches before weight update \n",
    "    # max_length=2048,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\":0.1},\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TRANSFORMERS_NO_TRACKIO\"] = \"1\"\n",
    "\n",
    "# # Force gradio to listen on all interfaces instead of just localhost\n",
    "# os.environ[\"GRADIO_SERVER_NAME\"] = \"0.0.0.0\"\n",
    "# os.environ[\"GRADIO_SERVER_PORT\"] = \"7860\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  Running in WANDB offline mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c28aa51e7b4bc89fdc5d6554d35bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Train the model\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "## Define the trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 13:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.788700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=0.3329375944440327, metrics={'train_runtime': 855.9272, 'train_samples_per_second': 1.168, 'train_steps_per_second': 0.074, 'total_flos': 1.2454801891048858e+17, 'train_loss': 0.3329375944440327})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TRAIN\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
