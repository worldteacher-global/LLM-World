{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://github.com/openai/gpt-oss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 225197.53it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:06<00:00, 22.33s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', # Tells the pipeline we would like to generate text\n",
    "    model='openai/gpt-oss-20b', # specifies the model\n",
    "    torch_dtype='auto', # data type the model uses in PyTorch\n",
    "    device_map='auto') # maps the model to a device\n",
    "\n",
    "messages = [{'role':'user', 'content':'Explain what mixture-of-experts is'}]\n",
    "\n",
    "out = pipe(messages,\n",
    " max_new_tokens=3000, # number of tokens the model will generate (output length)\n",
    " temperature=1.0) # 1 = baseline randomness in token selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'Explain what mixture-of-experts is'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['generated_text'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Explain what mixture-of-experts is'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['role', 'content'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Explain what mixture-of-experts is\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][0]['role'])\n",
    "print(out[0]['generated_text'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind's Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\n",
      "\n",
      "We'll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\n",
      "\n",
      "Given chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\n",
      "\n",
      "Let's compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\n",
      "\n",
      "At heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \n",
      "During training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\n",
      "\n",
      "* The combined capacity is far larger than any single expert.\n",
      "* Only a handful of experts are active for a given example—keeping compute cheap.\n",
      "* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\n",
      "\n",
      "### 1. Historical roots\n",
      "\n",
      "| Year | Paper | Core idea |\n",
      "|------|-------|------------|\n",
      "| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\n",
      "| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\n",
      "| 2017 | *“Switching Layers”* (Shazeer et al.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\n",
      "| 2020 | *“Switch Transformer”* + *“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\n",
      "\n",
      "### 2. Architecture in a nutshell\n",
      "\n",
      "```\n",
      "x  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\n",
      "                                              │      │      │        │ \n",
      "                                           out1   out2   …     oute\n",
      "                                              │      │      │        │ \n",
      "                                             Σ (gatei · outi) ──►  yhat\n",
      "```\n",
      "\n",
      "* **Input `x`** (text, image, etc.) goes into the *gate*.\n",
      "* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \n",
      "  * *Softmax* gate (dense) – every expert contributes a little.  \n",
      "  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\n",
      "* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\n",
      "* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\n",
      "\n",
      "#### Why sparsity?\n",
      "\n",
      "* **Compute:** Only `k ≪ E` experts are evaluated per token → ~k/E of the full computation.\n",
      "* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\n",
      "* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\n",
      "\n",
      "### 3. Training an MoE\n",
      "\n",
      "1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\n",
      "2. **Regularization on the gate** to prevent collapse into a single expert:\n",
      "   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\n",
      "   * **Entropy loss**: keep the distribution flat if you want diverse routing.\n",
      "   * **Auxiliary loss**: guide experts to output different representations.\n",
      "3. **Drop‑out or noise** can be applied to the gate to improve exploration.\n",
      "\n",
      "In practice, the training pipeline alternates between:\n",
      "  * Forward pass: compute `g`, select experts, evaluate, pool.\n",
      "  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\n",
      "\n",
      "### 4. Common Variants\n",
      "\n",
      "| Variant | Gate | Activation | Use‑case |\n",
      "|---------|------|------------|----------|\n",
      "| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\n",
      "| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\n",
      "| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1 × 10¹³ param BERT‑style model |\n",
      "| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\n",
      "| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\n",
      "| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\n",
      "\n",
      "### 5. Practical Applications\n",
      "\n",
      "| Domain | How MoE Helps | Example Model |\n",
      "|--------|--------------|---------------|\n",
      "| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\n",
      "| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\n",
      "| Speech | Accented / noisy variants | MoE conformer for ASR |\n",
      "| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\n",
      "| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\n",
      "| Reinforcement Learning | Policy specialization for states | MoE policy networks |\n",
      "\n",
      "### 6. Why is MoE attractive now?\n",
      "\n",
      "| Feature | Why it matters |\n",
      "|---------|----------------|\n",
      "| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\n",
      "| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\n",
      "| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\n",
      "| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\n",
      "| **Sparse routing** | Enables training very large models on commodity hardware. |\n",
      "\n",
      "### 7. Challenges & Open Issues\n",
      "\n",
      "| Issue | Impact | Mitigation |\n",
      "|-------|---------|------------|\n",
      "| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\n",
      "| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\n",
      "| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\n",
      "| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\n",
      "| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\n",
      "\n",
      "### 8. Quick pseudo‑code (PyTorch)\n",
      "\n",
      "```python\n",
      "class MoE(nn.Module):\n",
      "    def __init__(self, num_experts, hidden_dim, top_k=2):\n",
      "        super().__init__()\n",
      "        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\n",
      "        self.experts = nn.ModuleList(\n",
      "            [nn.Sequential(\n",
      "                nn.Linear(hidden_dim, hidden_dim),\n",
      "                nn.ReLU(),\n",
      "                nn.Linear(hidden_dim, hidden_dim))\n",
      "             for _ in range(num_experts)] )\n",
      "        self.top_k = top_k\n",
      "\n",
      "    def forward(self, x):\n",
      "        # x: (batch, hidden_dim)\n",
      "        gate_logits = self.gate(x)                     # (batch, E)\n",
      "        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\n",
      "\n",
      "        # sparse top‑k selection\n",
      "        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\n",
      "        sparse_gate         = torch.zeros_like(gate_logits).scatter(\n",
      "            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\n",
      "\n",
      "        # expert outputs (only top‑k evaluated)\n",
      "        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\n",
      "        # expert_outs: (batch, E, hidden_dim)\n",
      "\n",
      "        # weighted sum\n",
      "        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\n",
      "        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\n",
      "        return out\n",
      "```\n",
      "\n",
      "*In practice* you’d add:\n",
      "* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\n",
      "* **Switch entropy**: `-gate_probs.log().mean()`\n",
      "\n",
      "### 9. Take‑away\n",
      "\n",
      "* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\n",
      "* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\n",
      "* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\n",
      "* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\n",
      "\n",
      "Feel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][1]['role'])\n",
    "print(out[0]['generated_text'][1]['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced inference .generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 208671.84it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:43<00:00, 34.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-15\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Explain what MXFP4 quantization is.<|end|><|start|>assistant<|channel|>analysis<|message|>The user asks: \"Explain what MXFP4 quantization is.\" They likely refer to some technology, maybe in audio or signal processing? \"MXFP4\" seems unfamiliar. Could be a specific standard or algorithm. Might refer to \"Mixed-Precision Fixed-Point 4\" or \"Maximum eXponential Fixed Precision 4\"? Or \"Matrix Factorization Pair 4\"? Or maybe \"Milton's eXperimental Fixed Pattern 4\"? I need to interpret. Search memory: \"MXFP4\" appears in the context of \"MXFP4 quantization\" maybe in deep learning or neural networks? There's \"MXFP\" stands for \"Mixed-Precision Floating Point\"? Actually \"MXFP\" could be \"Microsoft eXpress Forward Propagation\"? Not sure.\n",
      "\n",
      "Let's think: \"MXFP4\" might be a quantization scheme used by \"MXFP\" from Mathematically eXtracted Free Parameter 4?? Might be a mis-typed \"MX\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/gpt-oss-20b\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    " \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    " \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    " \n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    " \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e50be1eef4142778b645adff713f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual-Thinking Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Rreasoning dataset where the chain-of-thought has been translated into several languages such as French, Spanish, and German. By fine-tuning openai/gpt-oss-20b on this dataset, it will learn to generate reasoning steps in these languages, and thus its reasoning process can be interpreted by users who speak those languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmony Response Format\n",
    "\n",
    "# developer: The developer message is used to provide custom instructions for the model (what we usually call the system role).\n",
    "\n",
    "# user: The user message is used to provide the input to the model.\n",
    "\n",
    "# assistant: Output by the model which can either be a tool call or a message output. The output might also be associated with a particular “channel” identifying what the intent of the message is.\n",
    "\n",
    "# analysis: These are messages that are being used by the model for its chain-of-thought\n",
    "\n",
    "# final: Messages tagged in the final channel are messages intended to be shown to the end-user and represent the responses from the model.\n",
    "\n",
    "# messages: The list of messages that combine the content of the above to produce a full conversation. This is the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692f785b5b9c4f0abfb194e92cbbe277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dad0734c0994675a42cc637daf9bd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/5.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9026d518ac4ffcae2e643c1e33f7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'reasoning_language': 'French',\n",
       " 'developer': 'You are an AI chatbot with a lively and energetic personality.',\n",
       " 'user': 'Can you show me the latest trends on Twitter right now?',\n",
       " 'analysis': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\",\n",
       " 'final': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       " 'messages': [{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.',\n",
       "   'role': 'system',\n",
       "   'thinking': None},\n",
       "  {'content': 'Can you show me the latest trends on Twitter right now?',\n",
       "   'role': 'user',\n",
       "   'thinking': None},\n",
       "  {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       "   'role': 'assistant',\n",
       "   'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('HuggingFaceH4/Multilingual-Thinking', split='train')\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to fine-tune the model, we need to convert these messages into a format that the model can understand. In practice this is done by formatting each message with the model's chat template and then tokenizing the resulting text.\n",
    "\n",
    "# *** TRL library does this automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual input formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8874bb35ab7439ea9cb1f059d82b393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33567c063f30425b9453342b3cde21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d5c48f967b4e71924cf5e83660a7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706e0cde206444b3bf91d0273b5a90e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai/gpt-oss-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.', 'role': 'system', 'thinking': None}, {'content': 'Can you show me the latest trends on Twitter right now?', 'role': 'user', 'thinking': None}, {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!', 'role': 'assistant', 'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['messages']\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-16\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: French\n",
      "\n",
      "You are an AI chatbot with a lively and energetic personality.\n",
      "\n",
      "<|end|><|start|>user<|message|>Can you show me the latest trends on Twitter right now?<|end|><|start|>assistant<|channel|>analysis<|message|>D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\n",
      "\n",
      "Je devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.<|end|><|start|>assistant<|channel|>final<|message|>Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\n",
      "\n",
      "1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \n",
      "2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \n",
      "3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \n",
      "4. **Check regional trends** – they often differ by location!  \n",
      "\n",
      "Remember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \n",
      "\n",
      "Want me to brainstorm *what* might trend next? I’ve got ideas!<|return|>\n"
     ]
    }
   ],
   "source": [
    "conversation = tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190dac190e07490bbfb2962717be4a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0262d04d4a3840708cbeef14ce4b3923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Download the weights\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
    "\n",
    "\n",
    "quantization_config = Mxfp4Config(dequantize=True) # Loads the necessary config for training. Allows mixed precision training with a special 4-bit floating point format (MXFP4)\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'attn_implementation':'eager',\n",
    "#     'torch_dtype':torch.bfloat16,\n",
    "#     'quantization_config'=quantization_config,\n",
    "#     'use_cache':False,\n",
    "#     'device_map':'auto'\n",
    "#     }\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation='eager', # better performance\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    use_cache=False, # using gradient checkpointing (slectivley saving/recomputing intermediate activations during backpropagation instead of storing them all)\n",
    "    device_map='auto') \n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('openai/gpt-oss-20b',**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-17\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>¿Cuál es el capital de Australia?<|end|><|start|>assistant<|channel|>analysis<|message|>We need to answer in Spanish presumably. The question: \"¿Cuál es el capital de Australia?\" We answer: Canberra. Provide short answer.<|end|><|start|>assistant<|channel|>final<|message|>La capital de Australia es Canberra.<|return|>\n"
     ]
    }
   ],
   "source": [
    "# tokenize sample prompt\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "# Test untrained model on sample prompt\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use LoRA (Low-rank Adaptation) to fine-tune the model (fine-tune specific layers of the model)  \n",
    "  \n",
    "## Parameter Effiecient Fine-Tuning\n",
    "# Wrap model as PeftModel and define LoRA config\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                   # enable more trainable parameters\n",
    "    lora_alpha=16,\n",
    "    target_modules='all-linear', # layers to target ie: attn\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,040,512 || all params: 20,929,797,696 || trainable%: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model,peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tuning with Transformers Renforcment Learning (TRL)\n",
    "\n",
    "## Define hyperparameters\n",
    "\n",
    "from trl import SFTConfig\n",
    "\n",
    "filepath = '/home/sagemaker-user/user-default-efs/CLONED_REPOS/LLM-World'\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    learning_rate=2e-4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4, # accumulate gradients over multiple batches before weight update \n",
    "    max_length=2048,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\":0.1},\n",
    "    output_dir=os.path.joint(filepath,\"gpt-oss-20b-multilingual-reasoner\"),\n",
    "    report_to=\"trackio\",\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "## Define the trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: huggingface\n",
      "* Trackio metrics logged to: /home/sagemaker-user/.cache/huggingface/trackio\n",
      "* View dashboard by running in your terminal:\n",
      "\u001b[1m\u001b[93mtrackio show --project \"huggingface\"\u001b[0m\n",
      "* or by running in Python: trackio.show(project=\"huggingface\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.14 GiB. GPU 3 has a total capacity of 21.98 GiB of which 2.84 GiB is free. Process 13791 has 19.13 GiB memory in use. Of the allocated memory 18.28 GiB is allocated by PyTorch, and 554.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## TRAIN\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/trainer.py:2229\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2226\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2227\u001b[39m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[32m   2228\u001b[39m     hf_hub_utils.disable_progress_bars()\n\u001b[32m-> \u001b[39m\u001b[32m2229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2235\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2236\u001b[39m     hf_hub_utils.enable_progress_bars()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/trainer.py:2582\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:904\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/trainer.py:3796\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3795\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3796\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3798\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3800\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3801\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3802\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:858\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m \u001b[33;03mCompute training loss and additionally compute token accuracies\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    857\u001b[39m mode = \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.training \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m (loss, outputs) = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[32m    863\u001b[39m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n\u001b[32m    864\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/trainer.py:3884\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3882\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3883\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3885\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3886\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/peft/peft_model.py:881\u001b[39m, in \u001b[36mPeftModel.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    880\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:677\u001b[39m, in \u001b[36mGptOssForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    675\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    679\u001b[39m aux_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_router_logits:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/loss/loss_utils.py:67\u001b[39m, in \u001b[36mForCausalLMLoss\u001b[39m\u001b[34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[32m     66\u001b[39m shift_labels = shift_labels.to(logits.device)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m loss = \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/loss/loss_utils.py:36\u001b[39m, in \u001b[36mfixed_cross_entropy\u001b[39m\u001b[34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfixed_cross_entropy\u001b[39m(\n\u001b[32m     29\u001b[39m     source: torch.Tensor,\n\u001b[32m     30\u001b[39m     target: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     **kwargs,\n\u001b[32m     34\u001b[39m ) -> torch.Tensor:\n\u001b[32m     35\u001b[39m     reduction = \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reduction == \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[39;00m\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(num_items_in_batch):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/torch/nn/functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 6.14 GiB. GPU 3 has a total capacity of 21.98 GiB of which 2.84 GiB is free. Process 13791 has 19.13 GiB memory in use. Of the allocated memory 18.28 GiB is allocated by PyTorch, and 554.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "## TRAIN\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
