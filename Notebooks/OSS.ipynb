{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://github.com/openai/gpt-oss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 225197.53it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:06<00:00, 22.33s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', # Tells the pipeline we would like to generate text\n",
    "    model='openai/gpt-oss-20b', # specifies the model\n",
    "    torch_dtype='auto', # data type the model uses in PyTorch\n",
    "    device_map='auto') # maps the model to a device\n",
    "\n",
    "messages = [{'role':'user', 'content':'Explain what mixture-of-experts is'}]\n",
    "\n",
    "out = pipe(messages,\n",
    " max_new_tokens=3000, # number of tokens the model will generate (output length)\n",
    " temperature=1.0) # 1 = baseline randomness in token selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'user',\n",
       "    'content': 'Explain what mixture-of-experts is'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['generated_text'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Explain what mixture-of-experts is'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind\\'s Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\\n\\nWe\\'ll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\\n\\nGiven chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\\n\\nLet\\'s compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\\n\\nAt heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \\nDuring training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\\n\\n* The combined capacity is far larger than any single expert.\\n* Only a handful of experts are active for a given example—keeping compute cheap.\\n* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\\n\\n### 1. Historical roots\\n\\n| Year | Paper | Core idea |\\n|------|-------|------------|\\n| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\\n| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\\n| 2017 | *“Switching Layers”* (Shazeer et\\u202fal.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\\n| 2020 | *“Switch Transformer”* +\\u202f*“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\\n\\n### 2. Architecture in a nutshell\\n\\n```\\nx  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\\n                                              │      │      │        │ \\n                                           out1   out2   …     oute\\n                                              │      │      │        │ \\n                                             Σ (gatei · outi) ──►  yhat\\n```\\n\\n* **Input `x`** (text, image, etc.) goes into the *gate*.\\n* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \\n  * *Softmax* gate (dense) – every expert contributes a little.  \\n  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\\n* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\\n* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\\n\\n#### Why sparsity?\\n\\n* **Compute:** Only `k\\u202f≪\\u202fE` experts are evaluated per token → ~k/E of the full computation.\\n* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\\n* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\\n\\n### 3. Training an MoE\\n\\n1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\\n2. **Regularization on the gate** to prevent collapse into a single expert:\\n   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\\n   * **Entropy loss**: keep the distribution flat if you want diverse routing.\\n   * **Auxiliary loss**: guide experts to output different representations.\\n3. **Drop‑out or noise** can be applied to the gate to improve exploration.\\n\\nIn practice, the training pipeline alternates between:\\n  * Forward pass: compute `g`, select experts, evaluate, pool.\\n  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\\n\\n### 4. Common Variants\\n\\n| Variant | Gate | Activation | Use‑case |\\n|---------|------|------------|----------|\\n| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\\n| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\\n| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1\\u202f×\\u202f10¹³ param BERT‑style model |\\n| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\\n| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\\n| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\\n\\n### 5. Practical Applications\\n\\n| Domain | How MoE Helps | Example Model |\\n|--------|--------------|---------------|\\n| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\\n| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\\n| Speech | Accented / noisy variants | MoE conformer for ASR |\\n| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\\n| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\\n| Reinforcement Learning | Policy specialization for states | MoE policy networks |\\n\\n### 6. Why is MoE attractive now?\\n\\n| Feature | Why it matters |\\n|---------|----------------|\\n| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\\n| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\\n| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\\n| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\\n| **Sparse routing** | Enables training very large models on commodity hardware. |\\n\\n### 7. Challenges & Open Issues\\n\\n| Issue | Impact | Mitigation |\\n|-------|---------|------------|\\n| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\\n| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\\n| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\\n| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\\n| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\\n\\n### 8. Quick pseudo‑code (PyTorch)\\n\\n```python\\nclass MoE(nn.Module):\\n    def __init__(self, num_experts, hidden_dim, top_k=2):\\n        super().__init__()\\n        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\\n        self.experts = nn.ModuleList(\\n            [nn.Sequential(\\n                nn.Linear(hidden_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Linear(hidden_dim, hidden_dim))\\n             for _ in range(num_experts)] )\\n        self.top_k = top_k\\n\\n    def forward(self, x):\\n        # x: (batch, hidden_dim)\\n        gate_logits = self.gate(x)                     # (batch, E)\\n        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\\n\\n        # sparse top‑k selection\\n        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\\n        sparse_gate         = torch.zeros_like(gate_logits).scatter(\\n            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\\n\\n        # expert outputs (only top‑k evaluated)\\n        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\\n        # expert_outs: (batch, E, hidden_dim)\\n\\n        # weighted sum\\n        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\\n        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\\n        return out\\n```\\n\\n*In practice* you’d add:\\n* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\\n* **Switch entropy**: `-gate_probs.log().mean()`\\n\\n### 9. Take‑away\\n\\n* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\\n* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\\n* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\\n* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\\n\\nFeel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['role', 'content'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['generated_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Explain what mixture-of-experts is\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][0]['role'])\n",
    "print(out[0]['generated_text'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "analysisThe user asks: \"Explain what mixture-of-experts is\". Need to provide a clear explanation of the concept: mixture of experts (MoE) architecture in machine learning, mixture models, gating networks, etc. Provide context, how it works, benefits, types, common applications (language modeling, vision), implementations (DeepMind's Switch Transformer). Provide a definition, explain components (experts, gate, sparsity), training objectives, and talk about inference and scaling. Provide example architecture: a feedforward network with multiple expert sub-networks and a gating network that assigns a probability vector over experts per input. The system uses mixture-of-experts to combine multiple specialized models. Provide references. Provide overview of training: cross-entropy loss, load balancing, regularization (entropy). Also talk about parameter efficiency, sparse activation. Provide examples like \"Mixture of Softmaxes\" for language modeling, \"Switch Transformer\". Maybe mention mixture of Gaussians as a simpler concept? But since user likely wants machine learning MoE.\n",
      "\n",
      "We'll produce a thorough but concise explanation, with bullet points, diagrams explained in text. Possibly mention \"Mixture of Experts\" first introduced by Jacobs et al. 1991. Provide context: use in mixture of models, gating. Offer example architecture: input processed by gating, each expert processes, outputs, weighted sum. Provide details on training: teacher forcing, etc. Provide note on \"Mixture of softmaxes\" tackling softmax bottleneck. Provide typical use-case: large language models, speech recognition, vision, robust to large parameter budgets. Mention the concept of \"sparsely activated\" MoE, where only a few experts used per input. Provide references to \"Sparse Mixture of Experts\" by Shazeer 2017, \"Switch Transformer\" 2020, \"GShard\" 2020. Provide notes on inference with gating. Provide discussion of benefits: capacity increase, efficient scaling, specialized sub-networks, modularity. Also discuss potential pitfalls: data imbalance, training complexity, memory, communication. Provide possible enhancements: load balancing, regularization, \"Top-K\" gating, \"routing\".\n",
      "\n",
      "Given chat context, we should tailor as an explanatory answer with clear structure: definition, components, training strategy, benefits, challenges, real-world examples, and resources. Provide bullet list at the end.\n",
      "\n",
      "Let's compose.assistantfinal## What is a **Mixture‑of‑Experts (MoE)**?\n",
      "\n",
      "At heart a mixture‑of‑experts is a *model‑composition* trick: instead of training one huge monolithic network, you build a **collection of smaller, specialized model components (“experts]\") and a tiny *“gate”* that learns to pick the right expert for each input.  \n",
      "During training and inference the system **mixes** the experts’ outputs according to the gate’s weights, ideally so that:\n",
      "\n",
      "* The combined capacity is far larger than any single expert.\n",
      "* Only a handful of experts are active for a given example—keeping compute cheap.\n",
      "* Each expert can learn to specialize on a subset of the data space (e.g., particular topics, languages, image styles, etc.).\n",
      "\n",
      "### 1. Historical roots\n",
      "\n",
      "| Year | Paper | Core idea |\n",
      "|------|-------|------------|\n",
      "| 1991 | *“Learning to Decompose…”* (Jacobs et al.) | First MoE for regression/classification – a linear *gating* network selects among neural‐network experts. |\n",
      "| 2016 | *“Mixture of Softmaxes”* (Yoshii & Sakoe) | Addressed the softmax bottleneck in language models. |\n",
      "| 2017 | *“Switching Layers”* (Shazeer et al.) | Introduced **sparsely activated** MoE for scaling language models (only 1–2 experts per example). |\n",
      "| 2020 | *“Switch Transformer”* + *“GShard”* (Google) | Paved the way for billions‑parameter MoE transformers used in production BERT/Transformer‑XL variants. |\n",
      "\n",
      "### 2. Architecture in a nutshell\n",
      "\n",
      "```\n",
      "x  ──►  gate (e.g., softmax or top‑k)  ──►  [expert1, expert2, …, expertE]\n",
      "                                              │      │      │        │ \n",
      "                                           out1   out2   …     oute\n",
      "                                              │      │      │        │ \n",
      "                                             Σ (gatei · outi) ──►  yhat\n",
      "```\n",
      "\n",
      "* **Input `x`** (text, image, etc.) goes into the *gate*.\n",
      "* The *gate* outputs a probability vector `g = [g₁, g₂, …, g_E]` over `E` experts.  \n",
      "  * *Softmax* gate (dense) – every expert contributes a little.  \n",
      "  * *Top‑K* or *Switch* gate (sparse) – only the top‑k experts get `g_i = 1`; the rest get 0.\n",
      "* Each expert is itself a neural sub‑network (often a feed‑forward block or a tiny transformer).\n",
      "* The final prediction `ŷ` is a (weighted) sum of the experts’ outputs.\n",
      "\n",
      "#### Why sparsity?\n",
      "\n",
      "* **Compute:** Only `k ≪ E` experts are evaluated per token → ~k/E of the full computation.\n",
      "* **Memory‑Efficient Scaling:** You can add *a lot* of experts (many parameters) without increasing per‑example FLOPs.\n",
      "* **Specialization:** Experts can drift to different “topics”; a data‑driven router keeps usage balanced.\n",
      "\n",
      "### 3. Training an MoE\n",
      "\n",
      "1. **Standard objective** (e.g., cross‑entropy for classification) on the *mixture* output.\n",
      "2. **Regularization on the gate** to prevent collapse into a single expert:\n",
      "   * **Load‑balancing loss**: encourage all experts to be used roughly equally.\n",
      "   * **Entropy loss**: keep the distribution flat if you want diverse routing.\n",
      "   * **Auxiliary loss**: guide experts to output different representations.\n",
      "3. **Drop‑out or noise** can be applied to the gate to improve exploration.\n",
      "\n",
      "In practice, the training pipeline alternates between:\n",
      "  * Forward pass: compute `g`, select experts, evaluate, pool.\n",
      "  * Backward pass: gradients flow simultaneously through the gate *and* the selected experts.\n",
      "\n",
      "### 4. Common Variants\n",
      "\n",
      "| Variant | Gate | Activation | Use‑case |\n",
      "|---------|------|------------|----------|\n",
      "| **Dense MoE** | Softmax | All experts considered | Traditional “Mixture of Experts” models |\n",
      "| **Sparse MoE** | Softmax + `top‑k` | Only a few experts | Large‑scale LMs (Switch Transformer) |\n",
      "| **Switch Transformer** | One‑hot `top‑1` gate | Each token routed to a single expert | 1 × 10¹³ param BERT‑style model |\n",
      "| **GShard** | Mixture of experts with block‑soft‑max | Block‑sparse | Training across many TPU islands |\n",
      "| **Mixture of Softmaxes** | Softmax | Weighted log‑sum‑exp | Language modeling w/ more expressive output distribution |\n",
      "| **Adaptive Softmax + MoE** | Data‑dependent routing | Hierarchical | Efficient large‑vocabulary training |\n",
      "\n",
      "### 5. Practical Applications\n",
      "\n",
      "| Domain | How MoE Helps | Example Model |\n",
      "|--------|--------------|---------------|\n",
      "| Natural Language | Large vocabularies, long‑context | SwitchTransformer, GShard |\n",
      "| Computer Vision | Diverse image styles / domains | MoE Vision Transformer (ViT‑MoE) |\n",
      "| Speech | Accented / noisy variants | MoE conformer for ASR |\n",
      "| Recommendation | Multi‑domain user interests | MoE recommenders (lightweight and accurate) |\n",
      "| Multilingual | Language‑specific sub‑networks | Cross‑lingual GPT‑MoE |\n",
      "| Reinforcement Learning | Policy specialization for states | MoE policy networks |\n",
      "\n",
      "### 6. Why is MoE attractive now?\n",
      "\n",
      "| Feature | Why it matters |\n",
      "|---------|----------------|\n",
      "| **Scalability** | Add more experts → more capacity, without blowing up per‑step cost. |\n",
      "| **Parameter efficiency** | Shared backbone, expert sub‑networks, minimal overhead. |\n",
      "| **Specialization** | Experts can learn distinct \"skills\" (topics, languages). |\n",
      "| **Modularity** | Easier to swap in/out experts, fine‑tune subsets. |\n",
      "| **Sparse routing** | Enables training very large models on commodity hardware. |\n",
      "\n",
      "### 7. Challenges & Open Issues\n",
      "\n",
      "| Issue | Impact | Mitigation |\n",
      "|-------|---------|------------|\n",
      "| **Data imbalance** | Some experts under‑/over‑used | Load‑balancing regularization, temperature tuning |\n",
      "| **Routing noise** | Can hurt gradient signal | Gumbel‑softmax trick, annealing |\n",
      "| **Memory fragmentation** | Efficient GPU memory use | Packed expert tensors, contiguous memory |\n",
      "| **Training overhead** | Extra parameters, communication | Pipeline parallelism, sharding |\n",
      "| **Interpretability** | Harder to interpret routing | Expert‑usage logs, explainability modules |\n",
      "\n",
      "### 8. Quick pseudo‑code (PyTorch)\n",
      "\n",
      "```python\n",
      "class MoE(nn.Module):\n",
      "    def __init__(self, num_experts, hidden_dim, top_k=2):\n",
      "        super().__init__()\n",
      "        self.gate = nn.Linear(hidden_dim, num_experts)   # softmax gate\n",
      "        self.experts = nn.ModuleList(\n",
      "            [nn.Sequential(\n",
      "                nn.Linear(hidden_dim, hidden_dim),\n",
      "                nn.ReLU(),\n",
      "                nn.Linear(hidden_dim, hidden_dim))\n",
      "             for _ in range(num_experts)] )\n",
      "        self.top_k = top_k\n",
      "\n",
      "    def forward(self, x):\n",
      "        # x: (batch, hidden_dim)\n",
      "        gate_logits = self.gate(x)                     # (batch, E)\n",
      "        probs      = F.softmax(gate_logits, dim=1)      # (batch, E)\n",
      "\n",
      "        # sparse top‑k selection\n",
      "        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=1)\n",
      "        sparse_gate         = torch.zeros_like(gate_logits).scatter(\n",
      "            1, topk_idx, 1.0 / self.top_k)              # one‑hot with scaling\n",
      "\n",
      "        # expert outputs (only top‑k evaluated)\n",
      "        expert_outs = torch.stack([exp(x) for exp in self.experts], dim=1)\n",
      "        # expert_outs: (batch, E, hidden_dim)\n",
      "\n",
      "        # weighted sum\n",
      "        gate_expanded = sparse_gate.unsqueeze(-1)        # (batch, E, 1)\n",
      "        out = (gate_expanded * expert_outs).sum(1)        # (batch, hidden_dim)\n",
      "        return out\n",
      "```\n",
      "\n",
      "*In practice* you’d add:\n",
      "* **Load‑balance loss**: `(gate_probs.mean(0) * gate_probs).sum()`\n",
      "* **Switch entropy**: `-gate_probs.log().mean()`\n",
      "\n",
      "### 9. Take‑away\n",
      "\n",
      "* **Mixture‑of‑Experts** is a strategy for building huge, expressive models that keep the per‑example cost modest by activating only a small subset of experts.\n",
      "* It’s the core of modern giant language models (Switch Transformer, GShard) and is rapidly finding use in other domains.\n",
      "* The key ingredients: a gating network, a pool of specialist experts, and careful training regularization to balance expert usage.\n",
      "* When used right, MoE gives you *more capacity at less cost* while also endowing the system with an emergent form of modularity and interpretability.\n",
      "\n",
      "Feel free to ask if you’d like deeper dives into a particular variant, implementation tricks, or a link to open‑source code!\n"
     ]
    }
   ],
   "source": [
    "print(out[0]['generated_text'][1]['role'])\n",
    "print(out[0]['generated_text'][1]['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced inference .generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 40 files: 100%|██████████| 40/40 [00:00<00:00, 208671.84it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:43<00:00, 34.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-15\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Explain what MXFP4 quantization is.<|end|><|start|>assistant<|channel|>analysis<|message|>The user asks: \"Explain what MXFP4 quantization is.\" They likely refer to some technology, maybe in audio or signal processing? \"MXFP4\" seems unfamiliar. Could be a specific standard or algorithm. Might refer to \"Mixed-Precision Fixed-Point 4\" or \"Maximum eXponential Fixed Precision 4\"? Or \"Matrix Factorization Pair 4\"? Or maybe \"Milton's eXperimental Fixed Pattern 4\"? I need to interpret. Search memory: \"MXFP4\" appears in the context of \"MXFP4 quantization\" maybe in deep learning or neural networks? There's \"MXFP\" stands for \"Mixed-Precision Floating Point\"? Actually \"MXFP\" could be \"Microsoft eXpress Forward Propagation\"? Not sure.\n",
      "\n",
      "Let's think: \"MXFP4\" might be a quantization scheme used by \"MXFP\" from Mathematically eXtracted Free Parameter 4?? Might be a mis-typed \"MX\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/gpt-oss-20b\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    " \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"},\n",
    "]\n",
    " \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    " \n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    " \n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual-Thinking Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Rreasoning dataset where the chain-of-thought has been translated into several languages such as French, Spanish, and German. By fine-tuning openai/gpt-oss-20b on this dataset, it will learn to generate reasoning steps in these languages, and thus its reasoning process can be interpreted by users who speak those languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmony Response Format\n",
    "\n",
    "# developer: The developer message is used to provide custom instructions for the model (what we usually call the system role).\n",
    "\n",
    "# user: The user message is used to provide the input to the model.\n",
    "\n",
    "# assistant: Output by the model which can either be a tool call or a message output. The output might also be associated with a particular “channel” identifying what the intent of the message is.\n",
    "\n",
    "# analysis: These are messages that are being used by the model for its chain-of-thought\n",
    "\n",
    "# final: Messages tagged in the final channel are messages intended to be shown to the end-user and represent the responses from the model.\n",
    "\n",
    "# messages: The list of messages that combine the content of the above to produce a full conversation. This is the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning_language': 'French',\n",
       " 'developer': 'You are an AI chatbot with a lively and energetic personality.',\n",
       " 'user': 'Can you show me the latest trends on Twitter right now?',\n",
       " 'analysis': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\",\n",
       " 'final': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       " 'messages': [{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.',\n",
       "   'role': 'system',\n",
       "   'thinking': None},\n",
       "  {'content': 'Can you show me the latest trends on Twitter right now?',\n",
       "   'role': 'user',\n",
       "   'thinking': None},\n",
       "  {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       "   'role': 'assistant',\n",
       "   'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('HuggingFaceH4/Multilingual-Thinking', split='train')\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to fine-tune the model, we need to convert these messages into a format that the model can understand. In practice this is done by formatting each message with the model's chat template and then tokenizing the resulting text.\n",
    "\n",
    "# *** TRL library does this automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual input formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai/gpt-oss-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.', 'role': 'system', 'thinking': None}, {'content': 'Can you show me the latest trends on Twitter right now?', 'role': 'user', 'thinking': None}, {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!', 'role': 'assistant', 'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['messages']\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-16\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: French\n",
      "\n",
      "You are an AI chatbot with a lively and energetic personality.\n",
      "\n",
      "<|end|><|start|>user<|message|>Can you show me the latest trends on Twitter right now?<|end|><|start|>assistant<|channel|>analysis<|message|>D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\n",
      "\n",
      "Je devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.<|end|><|start|>assistant<|channel|>final<|message|>Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\n",
      "\n",
      "1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \n",
      "2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \n",
      "3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \n",
      "4. **Check regional trends** – they often differ by location!  \n",
      "\n",
      "Remember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \n",
      "\n",
      "Want me to brainstorm *what* might trend next? I’ve got ideas!<|return|>\n"
     ]
    }
   ],
   "source": [
    "conversation = tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d56148dc90f4be18937b6b8a42c29b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.57 GiB of which 1.88 GiB is free. Process 16906 has 12.69 GiB memory in use. Of the allocated memory 10.15 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# model_kwargs = {\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#     'attn_implementation':'eager',\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#     'torch_dtype':torch.bfloat16,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#     'device_map':'auto'\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[32m     17\u001b[39m model_kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     18\u001b[39m     attn_implementation=\u001b[33m'\u001b[39m\u001b[33meager\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m     torch_dtype=torch.bfloat16,\n\u001b[32m     20\u001b[39m     quantization_config=quantization_config,\n\u001b[32m     21\u001b[39m     use_cache=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     22\u001b[39m     device_map=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m) \n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenai/gpt-oss-20b\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/modeling_utils.py:317\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/modeling_utils.py:5069\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5059\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5060\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5062\u001b[39m     (\n\u001b[32m   5063\u001b[39m         model,\n\u001b[32m   5064\u001b[39m         missing_keys,\n\u001b[32m   5065\u001b[39m         unexpected_keys,\n\u001b[32m   5066\u001b[39m         mismatched_keys,\n\u001b[32m   5067\u001b[39m         offload_index,\n\u001b[32m   5068\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5069\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5070\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5071\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5072\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5074\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5075\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5076\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5078\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5079\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5080\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5081\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5082\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5083\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5084\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5085\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5086\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5087\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/modeling_utils.py:5532\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5529\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5531\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5532\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5533\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5535\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/modeling_utils.py:975\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/modeling_utils.py:883\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    880\u001b[39m     _load_parameter_into_model(model, param_name, param.to(param_device))\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[32m    888\u001b[39m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[32m    889\u001b[39m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[32m    890\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/quantizers/quantizer_mxfp4.py:247\u001b[39m, in \u001b[36mMxfp4HfQuantizer.create_quantized_param\u001b[39m\u001b[34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.quantization_config.dequantize:\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# dq_param_name is the name of the parameter without the blocks or scales suffix, it's used in this case since we don't switch linears\u001b[39;00m\n\u001b[32m    245\u001b[39m     \u001b[38;5;66;03m# so we only have the original param name\u001b[39;00m\n\u001b[32m    246\u001b[39m     dq_param_name = param_name[: -\u001b[38;5;28mlen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m_blocks\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43mdequantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdq_param_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mshard_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    249\u001b[39m     load_and_swizzle_mxfp4(\n\u001b[32m    250\u001b[39m         module,\n\u001b[32m    251\u001b[39m         param_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m         **shard_kwargs,\n\u001b[32m    255\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/integrations/mxfp4.py:342\u001b[39m, in \u001b[36mdequantize\u001b[39m\u001b[34m(module, param_name, param_value, target_device, dq_param_name, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28msetattr\u001b[39m(module, param_name.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m1\u001b[39m], param_value)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, blocks_attr) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, scales_attr):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     dequantized = \u001b[43mconvert_moe_packed_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocks_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m     dequantized = dequantized.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous().to(target_device)\n\u001b[32m    344\u001b[39m     \u001b[38;5;66;03m# TODO: this is perhaps necessary since if target_device is cpu, and the param was on gpu\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/custom-file-systems/efs/fs-0252e317d4af1dc34_fsap-0a708b50be80889d5/CLONED_REPOS/LLM-World/llmenv/lib/python3.12/site-packages/transformers/integrations/mxfp4.py:124\u001b[39m, in \u001b[36mconvert_moe_packed_tensors\u001b[39m\u001b[34m(blocks, scales, dtype, rows_per_chunk)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# nibble indices -> int64\u001b[39;00m\n\u001b[32m    123\u001b[39m idx_lo = (blk & \u001b[32m0x0F\u001b[39m).to(torch.long)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m idx_hi = \u001b[43m(\u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[43m>>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m sub = out[r0:r1]\n\u001b[32m    127\u001b[39m sub[:, \u001b[32m0\u001b[39m::\u001b[32m2\u001b[39m] = lut[idx_lo]\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 14.57 GiB of which 1.88 GiB is free. Process 16906 has 12.69 GiB memory in use. Of the allocated memory 10.15 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 1. Download the weights\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
    "\n",
    "\n",
    "quantization_config = Mxfp4Config(dequantize=True)\n",
    "\n",
    "# model_kwargs = {\n",
    "#     'attn_implementation':'eager',\n",
    "#     'torch_dtype':torch.bfloat16,\n",
    "#     'quantization_config'=quantization_config,\n",
    "#     'use_cache':False,\n",
    "#     'device_map':'auto'\n",
    "#     }\n",
    "\n",
    "model_kwargs = dict(\n",
    "    attn_implementation='eager',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    use_cache=False,\n",
    "    device_map='auto') \n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('openai/gpt-oss-20b',**model_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
