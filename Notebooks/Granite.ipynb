{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b433f8b-57ec-4b03-861c-4b574d7a7c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/cloned_repo/LLM-World/.venv/bin/python\n",
      "3.11.14 (main, Dec 17 2025, 21:07:37) [Clang 21.1.4 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff05264-7bf7-4592-97cd-b341e0a968cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  9 14:33:07 2026       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.6     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              59W / 400W |  40025MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              56W / 400W |     12MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              62W / 400W |  39006MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              53W / 400W |     12MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              63W / 400W |    528MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              57W / 400W |     68MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              60W / 400W |     12MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              61W / 400W |   2005MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d760b337-80c7-49cd-b6c5-27cd185d09a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/cloned_repo/LLM-World/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff262ff5-1458-4db6-beef-bc72c34cc184",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff246fa1-a140-4b41-a195-09e65c1345ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/cloned_repo/LLM-World/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b73f422-9875-46a0-80d8-a296ec55cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/cloned_repo/LLM-World/Files/training_set.jsonl\n",
      "/app/cloned_repo/LLM-World/Files/validation_set.jsonl\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "trn_data = %pwd\n",
    "trn_data = trn_data.replace('Notebooks','Files/training_set.jsonl')\n",
    "val_data = trn_data.replace('training_set','validation_set')\n",
    "print(trn_data)      \n",
    "print(val_data)      \n",
    "print(os.path.exists(trn_data))\n",
    "print(os.path.exists(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b973ca8f-5aa0-4e53-aa54-8e8418529924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 409\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert to huggingface dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\n",
    "    \"train\":trn_data,\n",
    "    \"valid\":val_data\n",
    "})\n",
    "data\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f4ee86-8a3c-4030-9845-af67d370c55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [[{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is a vector space in linear algebra?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'Can you explain eigenvalues in simple terms?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Eigenvalues are special numbers that show how a matrix stretches or shrinks vectors along certain directions.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What is the difference between variance and standard deviation?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Variance measures the average squared deviation from the mean, while standard deviation is the square root of variance, making it easier to interpret in the same units as the data.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What does it mean if two events are independent in probability?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'It means the occurrence of one event does not affect the probability of the other happening.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'How do you know if two vectors are orthogonal?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Two vectors are orthogonal if their dot product equals zero.'}]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae11543-568e-4c31-842d-deb7a6c72755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [[{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What is the difference between matrix multiplication and element-wise multiplication?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Matrix multiplication involves dot products between rows and columns, while element-wise multiplication multiplies corresponding entries directly.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is an orthogonal matrix?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'An orthogonal matrix is a square matrix whose rows and columns are orthonormal vectors. Its inverse is the same as its transpose.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is covariance in statistics?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Covariance measures how two variables change together. A positive covariance means they increase together, while a negative one means one increases as the other decreases.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What is the difference between expectation and variance?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Expectation is the average value of a random variable, while variance measures how much the values fluctuate around that expectation.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is the Gram-Schmidt process?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'The Gram-Schmidt process takes a set of vectors and produces an orthogonal (or orthonormal) set spanning the same space.'}]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['valid'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb21f64-a56b-4960-8201-6614668bdb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       " {'role': 'user', 'content': 'What is a vector space in linear algebra?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa6f10-b78a-4840-b83c-678668a28635",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de438abe-2c15-41ab-aef1-87baaef5a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/ibm-granite/granite-4.0-h-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c09237-0fba-4259-a787-70245a45d555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/cloned_repo/LLM-World/sm_output'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dir = %pwd\n",
    "outp_dir = file_dir.replace('Notebooks','sm_output')\n",
    "outp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5febb6c-4a93-4b78-b117-c2c9d9b7ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dbe61ac-27ef-4e7e-9a8b-dec03e6bef4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    }
   ],
   "source": [
    "model_id = \"ibm-granite/granite-4.0-h-1b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id) # Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f66c3914-306d-46ea-b9fd-895f4149d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialization (dict -> str) for ability for model to read\n",
    "def serialize_message(input_example):\n",
    "    chat_str = \"\"\n",
    "    for message in input_example['messages']:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        if role == \"system\":\n",
    "            chat_str += f\"System: {content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            chat_str += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            chat_str += f\"Assistant: {content}\\n\"\n",
    "    input_example[\"text\"] = chat_str\n",
    "    return input_example\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7f639-1ac5-4498-8e23-7192ab00968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the data\n",
    "def t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738e4510-8426-479a-86ee-4dddad75b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HugingFace Trainer (Basic setup)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=outp_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    # bf16=True,\n",
    "    use_cpu=True, # dont use gpu when setting up variables If not using LoRA\n",
    "    fp16=True    \n",
    ")\n",
    "\n",
    "# better for training 1B+ OOM error possible without\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model,lora_config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['valid'],\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ebb925-eaae-4a60-b003-6adb883fd108",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature: (input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position, logits_to_keep, kwargs, labels, label_ids, label). The following columns have been ignored: [messages]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m trainer.save_model(file_dir.replace(\u001b[33m'\u001b[39m\u001b[33mNotebooks\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33msm_artifacts\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/cloned_repo/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/cloned_repo/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer.py:2375\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2373\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2374\u001b[39m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2375\u001b[39m train_dataloader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fsdp_xla_v2_enabled:\n\u001b[32m   2377\u001b[39m     train_dataloader = tpu_spmd_dataloader(train_dataloader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/cloned_repo/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer.py:1140\u001b[39m, in \u001b[36mTrainer.get_train_dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1138\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTrainer: training requires a train_dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/cloned_repo/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer.py:1095\u001b[39m, in \u001b[36mTrainer._get_dataloader\u001b[39m\u001b[34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[39m\n\u001b[32m   1093\u001b[39m data_collator = \u001b[38;5;28mself\u001b[39m.data_collator\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, datasets.Dataset):\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m     dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     data_collator = \u001b[38;5;28mself\u001b[39m._get_collator_with_removed_columns(\u001b[38;5;28mself\u001b[39m.data_collator, description=description)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/cloned_repo/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer.py:1021\u001b[39m, in \u001b[36mTrainer._remove_unused_columns\u001b[39m\u001b[34m(self, dataset, description)\u001b[39m\n\u001b[32m   1019\u001b[39m columns = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset.column_names]\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo columns in the dataset match the model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms forward method signature: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1025\u001b[39m     )\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(datasets.__version__) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m1.4.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1028\u001b[39m     dataset.set_format(\n\u001b[32m   1029\u001b[39m         \u001b[38;5;28mtype\u001b[39m=dataset.format[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m], columns=columns, format_kwargs=dataset.format[\u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1030\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No columns in the dataset match the model's forward method signature: (input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position, logits_to_keep, kwargs, labels, label_ids, label). The following columns have been ignored: [messages]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(file_dir.replace('Notebooks','sm_artifacts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f0a3487-80c4-4dd3-bf1d-223d89531fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31577ccc-f3c6-433c-807f-5ef980787fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d764da-62fe-41b8-912d-e08a463d012c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
