{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b433f8b-57ec-4b03-861c-4b574d7a7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff05264-7bf7-4592-97cd-b341e0a968cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760b337-80c7-49cd-b6c5-27cd185d09a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff262ff5-1458-4db6-beef-bc72c34cc184",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff246fa1-a140-4b41-a195-09e65c1345ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73f422-9875-46a0-80d8-a296ec55cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = %pwd\n",
    "trn_data = trn_data.replace('Notebooks','Files/training_set.jsonl')\n",
    "val_data = trn_data.replace('training_set','validation_set')\n",
    "print(trn_data)      \n",
    "print(val_data)      \n",
    "print(os.path.exists(trn_data))\n",
    "print(os.path.exists(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973ca8f-5aa0-4e53-aa54-8e8418529924",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to huggingface dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\n",
    "    \"train\":trn_data,\n",
    "    \"valid\":val_data\n",
    "})\n",
    "data\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4ee86-8a3c-4030-9845-af67d370c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae11543-568e-4c31-842d-deb7a6c72755",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['valid'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb21f64-a56b-4960-8201-6614668bdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa6f10-b78a-4840-b83c-678668a28635",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de438abe-2c15-41ab-aef1-87baaef5a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/ibm-granite/granite-4.0-h-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c6701-f6c9-4a11-b46d-2e1a49738bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a8afd-8b04-445f-b030-b8131954cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('/app/cloned_repo/LLM-World/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54a8aa-4aa5-4f8e-a804-85d07f2ce7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=os.getenv('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c09237-0fba-4259-a787-70245a45d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = %pwd\n",
    "outp_dir = file_dir.replace('Notebooks','Files/sm_output')\n",
    "outp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5febb6c-4a93-4b78-b117-c2c9d9b7ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe61ac-27ef-4e7e-9a8b-dec03e6bef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm-granite/granite-4.0-h-1b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id) # Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c3914-306d-46ea-b9fd-895f4149d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialization (dict -> str) for ability for model to read (flatten)\n",
    "def serialize_message(input_example):\n",
    "    chat_str = \"\"\n",
    "    for message in input_example['messages']:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        if role == \"system\":\n",
    "            chat_str += f\"System: {content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            chat_str += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            chat_str += f\"Assistant: {content}\\n\"\n",
    "    input_example[\"text\"] = chat_str\n",
    "    return input_example\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7f639-1ac5-4498-8e23-7192ab00968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the data\n",
    "def tokenize_text(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59195f-4ca3-4d48-8721-8b5385eaad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(serialize_message)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51981500-db2a-4fa6-a58b-7b1771a4e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f60eb-cbc6-446c-a6ef-896865d9f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(tokenize_text,batched=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989975b-1bf7-4e5e-ad3e-9304c4303d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e4510-8426-479a-86ee-4dddad75b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HugingFace Trainer (Basic setup)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=outp_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    # bf16=True,\n",
    "    no_cuda=True, # dont use gpu when setting up variables If not using LoRA\n",
    "    # use_cpu=True, # dont use gpu when setting up variables If not using LoRA\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=None,\n",
    "    hub_token=None\n",
    ")\n",
    "\n",
    "# better for training 1B+ OOM error possible without\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model,lora_config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['valid'],\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e8b32-ef4e-452b-9ba8-74317ddba38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = file_dir.replace('Notebooks','Files/sm_artifacts')\n",
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ebb925-eaae-4a60-b003-6adb883fd108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(save_dir)\n",
    "trainer.tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a3487-80c4-4dd3-bf1d-223d89531fb2",
   "metadata": {},
   "source": [
    "## With TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40bf817-9f3e-4d33-8085-5387c3ceda41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbbbafb-ac8b-487d-9171-a43eaafadf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = %pwd\n",
    "trn_data = trn_data.replace('Notebooks','Files/training_set.jsonl')\n",
    "val_data = trn_data.replace('training_set','validation_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9775bb5-7cf5-4311-b2e4-c555718a9832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 409\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert to huggingface dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\n",
    "    \"train\":trn_data,\n",
    "    \"valid\":val_data\n",
    "})\n",
    "data\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4402b4-902a-47a6-add4-3c9135de9ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       " {'role': 'user', 'content': 'What is a vector space in linear algebra?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data['train']['messages']))\n",
    "data['train']['messages'][0] # Conversation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8afab6-f099-4437-9ffe-b3eab9adae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialization (dict -> str) for ability for model to read (flatten)\n",
    "def serialize_message(input_example):\n",
    "    chat_str = \"\"\n",
    "    for message in input_example['messages']:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        if role == \"system\":\n",
    "            chat_str += f\"System: {content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            chat_str += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            chat_str += f\"Assistant: {content}\\n\"\n",
    "    input_example[\"text\"] = chat_str\n",
    "    return input_example\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0bb9f6-5530-4055-b78f-d104d1cf0e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'text'],\n",
       "    num_rows: 409\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by conversation so chat template can be used (alpac)\n",
    "flat_train_data = data['train'].map(serialize_message)\n",
    "flat_train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e31a9b-cf9d-49dc-bd60-2277b6894e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'System: You are a mathematician who is specialized in linear algebra and also statistics.\\nUser: What is a vector space in linear algebra?\\nAssistant: A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(flat_train_data['text']))\n",
    "flat_train_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31577ccc-f3c6-433c-807f-5ef980787fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7aeedb9-8b0b-44fb-9c6e-4acb8dcc1a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 11:46:33.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSFT with TRL\u001b[0m\n",
      "\u001b[32m2026-01-17 11:46:33.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mLoading model: ibm-granite/granite-4.0-h-1b\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ibm-granite/granite-4.0-h-1b\"\n",
    "\n",
    "\n",
    "logger.info(\"SFT with TRL\")\n",
    "logger.info(f\"Loading model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d764da-62fe-41b8-912d-e08a463d012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 11:46:34.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mpad_token:<|pad|> (allows tokens to have the same length)\u001b[0m\n",
      "The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    }
   ],
   "source": [
    "model_id = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load Tokenizer\n",
    "logger.info(f\"pad_token:{tokenizer.pad_token} (allows tokens to have the same length)\")\n",
    "\n",
    "if tokenizer.pad_token is None:    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    logger.info(f\"set pad_token=eos_token:{tokenizer.eos_token} (allows tokens to have the same length)\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id,device_map=\"auto\",dtype=torch.bfloat16) # Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id) # Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d3b363-250d-403b-96e9-e4f5fd07983c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraniteMoeHybridForCausalLM(\n",
       "  (model): GraniteMoeHybridModel(\n",
       "    (embed_tokens): Embedding(100352, 1536, padding_idx=100256)\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6-14): 9 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16-24): 9 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (26-34): 9 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (35): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (36-39): 4 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcdbd9fb-80d6-4aad-a599-35372623063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False # required for gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2b606ab-0237-4bf3-a679-5dad833394d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05f3ce0f-0f00-4f41-b3f5-6e63b344eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GraniteMoeHybridForCausalLM(\n",
       "      (model): GraniteMoeHybridModel(\n",
       "        (embed_tokens): Embedding(100352, 1536, padding_idx=100256)\n",
       "        (layers): ModuleList(\n",
       "          (0-4): 5 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (5): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6-14): 9 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (15): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (16-24): 9 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (25): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (26-34): 9 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (35): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (36-39): 4 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=100352, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "pt_model = get_peft_model(model,peft_config)\n",
    "pt_model.gradient_checkpointing_enable()\n",
    "pt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07c2c80c-22e1-494f-8710-4933df04c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 11:47:49.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTrainable parameters: 655,360/1,462,193,728 (0.04%)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655360\n",
      "1462193728\n"
     ]
    }
   ],
   "source": [
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in pt_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in pt_model.parameters())\n",
    "\n",
    "\n",
    "print(trainable_params)\n",
    "print(total_params)\n",
    "\n",
    "logger.info(f\"Trainable parameters: {trainable_params:,}/{total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9b8c26b-c9d5-4490-9a10-0872f6cb6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd835211-1942-45c4-866b-f043990d599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 11:47:53.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1moutput_dir=/app/home/marfok/LLM-World/Files/granite_ckp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pwd_dir = %pwd\n",
    "file_dir = pwd_dir.replace('Notebooks','Files/granite_ckp')\n",
    "logger.info(f\"output_dir={file_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adcffa72-7b05-425f-a0ba-d127042e983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_training_args = SFTConfig(\n",
    "    output_dir=str(file_dir),\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-5,\n",
    "    max_length=2048,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",#\"wandb\",\n",
    "    packing=False,\n",
    "    assistant_only_loss=False\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = pt_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=flat_train_data,\n",
    "    # peft_config=peft_config,\n",
    "    args=sft_training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a301a63-c4f7-4360-a532-55613cde696d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [104/104 17:20, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.655600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.310500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=104, training_loss=3.911779146928054, metrics={'train_runtime': 1051.0639, 'train_samples_per_second': 1.557, 'train_steps_per_second': 0.099, 'total_flos': 1362585787904640.0, 'train_loss': 3.911779146928054})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931213d-0de6-473f-bbff-97069db86c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model.save_pretrained(file_dir)\n",
    "tokenizer.save_pretrained(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43bf26-3d5b-45ab-8a0e-599a73d1b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_info = {\n",
    "    \"model_name\":model_name,\n",
    "    \"output_dir\":file_dir,\n",
    "    \"num_epochs\":epochs,\n",
    "    \"total_examples\":len(dataset),\n",
    "    \"loar_config\":{\"r\":LORA_R,\"alpha\":LORA_ALPHA, \"target_modules\":TARGET_MODULES},\n",
    "    \"training_config\":{\"batch_size\":batch_size, \"gradient_accumulation\":gradient_accululation,\"learning_rate\":learning_rate,\"max_length\":max_length}\n",
    "    }\n",
    "\n",
    "info_path = file_dir/\"checkpont_info.json\"\n",
    "\n",
    "with open(info_path,\"w\") as f:\n",
    "    json.dump(checkpoint_info, f, indent=2)\n",
    "\n",
    "logger.info(f\"Checkpoint info saved to {info_path}\")\n",
    "\n",
    "logger.info(f\"Model saved to {file_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442f97a-72d0-4375-b302-dd307e6e74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.join(file_dir,'hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55823fbd-2d3e-4f48-bd03-9304e650a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "from peft import PeftModel\n",
    "chk_dir = pwd.replace\n",
    "ft_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164f59d-bbaa-4bbf-90c6-5cee726d8a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9a1df-b879-4b23-8583-fec86417700d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm-py.311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
