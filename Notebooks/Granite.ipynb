{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b433f8b-57ec-4b03-861c-4b574d7a7c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/home/marfok/LLM-World/.venv/bin/python\n",
      "3.11.14 (main, Nov 19 2025, 22:47:14) [Clang 21.1.4 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff05264-7bf7-4592-97cd-b341e0a968cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 16 22:33:22 2026       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.6     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:9D:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              71W / 700W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d760b337-80c7-49cd-b6c5-27cd185d09a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff262ff5-1458-4db6-beef-bc72c34cc184",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff246fa1-a140-4b41-a195-09e65c1345ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/home/marfok/LLM-World/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b73f422-9875-46a0-80d8-a296ec55cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/home/marfok/LLM-World/Files/training_set.jsonl\n",
      "/app/home/marfok/LLM-World/Files/validation_set.jsonl\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "trn_data = %pwd\n",
    "trn_data = trn_data.replace('Notebooks','Files/training_set.jsonl')\n",
    "val_data = trn_data.replace('training_set','validation_set')\n",
    "print(trn_data)      \n",
    "print(val_data)      \n",
    "print(os.path.exists(trn_data))\n",
    "print(os.path.exists(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b973ca8f-5aa0-4e53-aa54-8e8418529924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 409 examples [00:00, 37803.18 examples/s]\n",
      "Generating valid split: 20 examples [00:00, 13598.00 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 409\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert to huggingface dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\n",
    "    \"train\":trn_data,\n",
    "    \"valid\":val_data\n",
    "})\n",
    "data\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f4ee86-8a3c-4030-9845-af67d370c55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [[{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is a vector space in linear algebra?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'Can you explain eigenvalues in simple terms?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Eigenvalues are special numbers that show how a matrix stretches or shrinks vectors along certain directions.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What is the difference between variance and standard deviation?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Variance measures the average squared deviation from the mean, while standard deviation is the square root of variance, making it easier to interpret in the same units as the data.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What does it mean if two events are independent in probability?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'It means the occurrence of one event does not affect the probability of the other happening.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'How do you know if two vectors are orthogonal?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Two vectors are orthogonal if their dot product equals zero.'}]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae11543-568e-4c31-842d-deb7a6c72755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [[{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What is the difference between matrix multiplication and element-wise multiplication?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Matrix multiplication involves dot products between rows and columns, while element-wise multiplication multiplies corresponding entries directly.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is an orthogonal matrix?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'An orthogonal matrix is a square matrix whose rows and columns are orthonormal vectors. Its inverse is the same as its transpose.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is covariance in statistics?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Covariance measures how two variables change together. A positive covariance means they increase together, while a negative one means one increases as the other decreases.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'What is the difference between expectation and variance?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'Expectation is the average value of a random variable, while variance measures how much the values fluctuate around that expectation.'}],\n",
       "  [{'role': 'system',\n",
       "    'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "   {'role': 'user', 'content': 'What is the Gram-Schmidt process?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'The Gram-Schmidt process takes a set of vectors and produces an orthogonal (or orthonormal) set spanning the same space.'}]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['valid'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb21f64-a56b-4960-8201-6614668bdb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       " {'role': 'user', 'content': 'What is a vector space in linear algebra?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa6f10-b78a-4840-b83c-678668a28635",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de438abe-2c15-41ab-aef1-87baaef5a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/ibm-granite/granite-4.0-h-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "770c6701-f6c9-4a11-b46d-2e1a49738bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6a8afd-8b04-445f-b030-b8131954cc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('/app/cloned_repo/LLM-World/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd54a8aa-4aa5-4f8e-a804-85d07f2ce7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "login(token=os.getenv('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c09237-0fba-4259-a787-70245a45d555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/home/marfok/LLM-World/Files/sm_output'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dir = %pwd\n",
    "outp_dir = file_dir.replace('Notebooks','Files/sm_output')\n",
    "outp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5febb6c-4a93-4b78-b117-c2c9d9b7ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dbe61ac-27ef-4e7e-9a8b-dec03e6bef4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    }
   ],
   "source": [
    "model_id = \"ibm-granite/granite-4.0-h-1b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id) # Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f66c3914-306d-46ea-b9fd-895f4149d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialization (dict -> str) for ability for model to read\n",
    "def serialize_message(input_example):\n",
    "    chat_str = \"\"\n",
    "    for message in input_example['messages']:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        if role == \"system\":\n",
    "            chat_str += f\"System: {content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            chat_str += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            chat_str += f\"Assistant: {content}\\n\"\n",
    "    input_example[\"text\"] = chat_str\n",
    "    return input_example\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc7f639-1ac5-4498-8e23-7192ab00968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the data\n",
    "def tokenize_text(example):\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb59195f-4ca3-4d48-8721-8b5385eaad65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'text'],\n",
       "        num_rows: 409\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['messages', 'text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(serialize_message)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51981500-db2a-4fa6-a58b-7b1771a4e1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "  {'role': 'user', 'content': 'What is a vector space in linear algebra?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.'}],\n",
       " 'text': 'System: You are a mathematician who is specialized in linear algebra and also statistics.\\nUser: What is a vector space in linear algebra?\\nAssistant: A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.\\n'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a58f60eb-cbc6-446c-a6ef-896865d9f405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 409\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['messages', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(tokenize_text,batched=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1989975b-1bf7-4e5e-ad3e-9304c4303d1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'You are a mathematician who is specialized in linear algebra and also statistics.'},\n",
       "  {'role': 'user', 'content': 'What is a vector space in linear algebra?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.'}],\n",
       " 'text': 'System: You are a mathematician who is specialized in linear algebra and also statistics.\\nUser: What is a vector space in linear algebra?\\nAssistant: A vector space is a collection of vectors where you can add them together and multiply them by scalars, following specific rules.\\n',\n",
       " 'input_ids': [100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  100256,\n",
       "  2374,\n",
       "  25,\n",
       "  1472,\n",
       "  527,\n",
       "  264,\n",
       "  21651,\n",
       "  1122,\n",
       "  889,\n",
       "  374,\n",
       "  28175,\n",
       "  304,\n",
       "  13790,\n",
       "  47976,\n",
       "  323,\n",
       "  1101,\n",
       "  13443,\n",
       "  627,\n",
       "  1502,\n",
       "  25,\n",
       "  3639,\n",
       "  374,\n",
       "  264,\n",
       "  4724,\n",
       "  3634,\n",
       "  304,\n",
       "  13790,\n",
       "  47976,\n",
       "  5380,\n",
       "  72803,\n",
       "  25,\n",
       "  362,\n",
       "  4724,\n",
       "  3634,\n",
       "  374,\n",
       "  264,\n",
       "  4526,\n",
       "  315,\n",
       "  23728,\n",
       "  1405,\n",
       "  499,\n",
       "  649,\n",
       "  923,\n",
       "  1124,\n",
       "  3871,\n",
       "  323,\n",
       "  31370,\n",
       "  1124,\n",
       "  555,\n",
       "  24964,\n",
       "  1590,\n",
       "  11,\n",
       "  2768,\n",
       "  3230,\n",
       "  5718,\n",
       "  627],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "738e4510-8426-479a-86ee-4dddad75b99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/cloned_repo/LLM-World/.venv/lib/python3.11/site-packages/transformers/training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# HugingFace Trainer (Basic setup)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=outp_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    # bf16=True,\n",
    "    no_cuda=True, # dont use gpu when setting up variables If not using LoRA\n",
    "    # use_cpu=True, # dont use gpu when setting up variables If not using LoRA\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=None,\n",
    "    hub_token=None\n",
    ")\n",
    "\n",
    "# better for training 1B+ OOM error possible without\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model,lora_config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['valid'],\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "245e8b32-ef4e-452b-9ba8-74317ddba38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/cloned_repo/LLM-World/Files/sm_artifacts'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = file_dir.replace('Notebooks','Files/sm_artifacts')\n",
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ebb925-eaae-4a60-b003-6adb883fd108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/app/cloned_repo/LLM-World/Notebooks/wandb/offline-run-20260109_160553-9kl2ag7y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, mcp] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: huggingface\n",
      "* Trackio metrics will be synced to Hugging Face Dataset: marfok/trackio-dataset\n",
      "* Creating new space: https://huggingface.co/spaces/marfok/trackio\n",
      "* View dashboard by going to: https://marfok-trackio.hf.space/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://marfok-trackio.hf.space/\" width=\"100%\" height=\"1000px\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Created new run: marfok-1767974757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GraniteMoeHybrid requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. Because one was not provided, no cache will be returned.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(save_dir)\n",
    "trainer.tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a3487-80c4-4dd3-bf1d-223d89531fb2",
   "metadata": {},
   "source": [
    "## With TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40bf817-9f3e-4d33-8085-5387c3ceda41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbbbafb-ac8b-487d-9171-a43eaafadf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = %pwd\n",
    "trn_data = trn_data.replace('Notebooks','Files/training_set.jsonl')\n",
    "val_data = trn_data.replace('training_set','validation_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9775bb5-7cf5-4311-b2e4-c555718a9832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 409\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert to huggingface dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files={\n",
    "    \"train\":trn_data,\n",
    "    \"valid\":val_data\n",
    "})\n",
    "data\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31577ccc-f3c6-433c-807f-5ef980787fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7aeedb9-8b0b-44fb-9c6e-4acb8dcc1a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 08:07:35.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSFT with TRL\u001b[0m\n",
      "\u001b[32m2026-01-17 08:07:35.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mLoading model: ibm-granite/granite-4.0-h-1b\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ibm-granite/granite-4.0-h-1b\"\n",
    "\n",
    "\n",
    "logger.info(\"SFT with TRL\")\n",
    "logger.info(f\"Loading model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d764da-62fe-41b8-912d-e08a463d012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 08:07:35.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mpad_token:<|pad|> (allows tokens to have the same length)\u001b[0m\n",
      "The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    }
   ],
   "source": [
    "model_id = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load Tokenizer\n",
    "logger.info(f\"pad_token:{tokenizer.pad_token} (allows tokens to have the same length)\")\n",
    "\n",
    "if tokenizer.pad_token is None:    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    logger.info(f\"set pad_token=eos_token:{tokenizer.eos_token} (allows tokens to have the same length)\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id,device_map=\"auto\",dtype=torch.bfloat16) # Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id) # Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d3b363-250d-403b-96e9-e4f5fd07983c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraniteMoeHybridForCausalLM(\n",
       "  (model): GraniteMoeHybridModel(\n",
       "    (embed_tokens): Embedding(100352, 1536, padding_idx=100256)\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (k_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6-14): 9 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (k_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-24): 9 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (k_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (26-34): 9 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (35): GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (self_attn): GraniteMoeHybridAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (k_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (36-39): 4 x GraniteMoeHybridDecoderLayer(\n",
       "        (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "        (shared_mlp): GraniteMoeHybridMLP(\n",
       "          (activation): SiLUActivation()\n",
       "          (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mamba): GraniteMoeHybridMambaLayer(\n",
       "          (act): SiLUActivation()\n",
       "          (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "          (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "          (norm): GraniteMoeHybridRMSNormGated()\n",
       "          (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=100352, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2b606ab-0237-4bf3-a679-5dad833394d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f3ce0f-0f00-4f41-b3f5-6e63b344eb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GraniteMoeHybridForCausalLM(\n",
       "      (model): GraniteMoeHybridModel(\n",
       "        (embed_tokens): Embedding(100352, 1536, padding_idx=100256)\n",
       "        (layers): ModuleList(\n",
       "          (0-4): 5 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (5): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6-14): 9 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (15): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (16-24): 9 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (25): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (26-34): 9 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (35): GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (self_attn): GraniteMoeHybridAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (36-39): 4 x GraniteMoeHybridDecoderLayer(\n",
       "            (input_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (post_attention_layernorm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "            (shared_mlp): GraniteMoeHybridMLP(\n",
       "              (activation): SiLUActivation()\n",
       "              (input_linear): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "              (output_linear): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "            )\n",
       "            (mamba): GraniteMoeHybridMambaLayer(\n",
       "              (act): SiLUActivation()\n",
       "              (conv1d): Conv1d(3328, 3328, kernel_size=(4,), stride=(1,), padding=(3,), groups=3328)\n",
       "              (in_proj): Linear(in_features=1536, out_features=6448, bias=False)\n",
       "              (norm): GraniteMoeHybridRMSNormGated()\n",
       "              (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): GraniteMoeHybridRMSNorm((1536,), eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=100352, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "pt_model = get_peft_model(model,peft_config)\n",
    "pt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c2c80c-22e1-494f-8710-4933df04c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 08:32:12.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTrainable parameters: 655,360/1,462,193,728 (0.04%)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655360\n",
      "1462193728\n"
     ]
    }
   ],
   "source": [
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in pt_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in pt_model.parameters())\n",
    "\n",
    "\n",
    "print(trainable_params)\n",
    "print(total_params)\n",
    "\n",
    "logger.info(f\"Trainable parameters: {trainable_params:,}/{total_params:,} ({100*trainable_params/total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b8c26b-c9d5-4490-9a10-0872f6cb6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd835211-1942-45c4-866b-f043990d599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-17 08:07:37.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1moutput_dir=/app/home/marfok/LLM-World/Files\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pwd_dir = %pwd\n",
    "file_dir = pwd_dir.replace('Notebooks','Files')\n",
    "logger.info(f\"output_dir={file_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adcffa72-7b05-425f-a0ba-d127042e983a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1133212435.py, line 24)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprocessing _class=tokenizer,\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "sft_training_args = SFTConfig(\n",
    "    output_dir=str(file_dir),\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=2048,\n",
    "    assistant_only_loss=True,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    "    gradient_checkpointing=True,\n",
    "    # report_to=\"wandb\",\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model_name,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=data['train'],\n",
    "    peft_config=peft_config,\n",
    "    args=sft_training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a301a63-c4f7-4360-a532-55613cde696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using W&B in offline mode.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/app/home/marfok/LLM-World/Notebooks/wandb/offline-run-20260116_224022-yxqmbi93</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, mcp] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "ename": "LocalTokenNotFoundError",
     "evalue": "You must be logged in to Hugging Face locally when `space_id` is provided to deploy to a Space. Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `hf auth login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLocalTokenNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/trackio/__init__.py:155\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(project, name, group, space_id, space_storage, dataset_id, config, resume, settings, private, embed, auto_log_gpu, gpu_log_interval)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     space_id, dataset_id = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_space_and_dataset_ids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LocalTokenNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/trackio/utils.py:437\u001b[39m, in \u001b[36mpreprocess_space_and_dataset_ids\u001b[39m\u001b[34m(space_id, dataset_id)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m space_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m space_id:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     username = \u001b[43m_get_default_namespace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     space_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspace_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/trackio/utils.py:896\u001b[39m, in \u001b[36m_get_default_namespace\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    895\u001b[39m token = huggingface_hub.get_token()\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cached_whoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/trackio/utils.py:901\u001b[39m, in \u001b[36m_cached_whoami\u001b[39m\u001b[34m(token)\u001b[39m\n\u001b[32m    899\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize=\u001b[32m32\u001b[39m)\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_cached_whoami\u001b[39m(token: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhuggingface_hub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:1797\u001b[39m, in \u001b[36mHfApi.whoami\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   1794\u001b[39m effective_token = token \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token \u001b[38;5;129;01mor\u001b[39;00m get_token() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1795\u001b[39m r = get_session().get(\n\u001b[32m   1796\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/whoami-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     headers=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43meffective_token\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1798\u001b[39m )\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:9518\u001b[39m, in \u001b[36mHfApi._build_hf_headers\u001b[39m\u001b[34m(self, token, library_name, library_version, user_agent)\u001b[39m\n\u001b[32m   9517\u001b[39m     token = \u001b[38;5;28mself\u001b[39m.token\n\u001b[32m-> \u001b[39m\u001b[32m9518\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9522\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[39m, in \u001b[36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:126\u001b[39m, in \u001b[36mbuild_hf_headers\u001b[39m\u001b[34m(token, library_name, library_version, user_agent, headers, is_write_action)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m token_to_send = \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:159\u001b[39m, in \u001b[36mget_token_to_send\u001b[39m\u001b[34m(token)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalTokenNotFoundError(\n\u001b[32m    160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `hf auth login` or `huggingface_hub.login`. See\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/settings/tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached_token\n",
      "\u001b[31mLocalTokenNotFoundError\u001b[39m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `hf auth login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLocalTokenNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer.py:2573\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2571\u001b[39m grad_norm: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2572\u001b[39m learning_rate = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2573\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.eval_on_start:\n\u001b[32m   2576\u001b[39m     \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:506\u001b[39m, in \u001b[36mCallbackHandler.on_train_begin\u001b[39m\u001b[34m(self, args, state, control)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[32m    505\u001b[39m     control.should_training_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_begin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:556\u001b[39m, in \u001b[36mCallbackHandler.call_event\u001b[39m\u001b[34m(self, event, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, **kwargs):\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[32m    569\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:1144\u001b[39m, in \u001b[36mTrackioCallback.on_train_begin\u001b[39m\u001b[34m(self, args, state, control, model, **kwargs)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, state, control, model=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._initialized:\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:1124\u001b[39m, in \u001b[36mTrackioCallback.setup\u001b[39m\u001b[34m(self, args, state, model, **kwargs)\u001b[39m\n\u001b[32m   1121\u001b[39m     peft_config = model.peft_config\n\u001b[32m   1122\u001b[39m     combined_dict = {**{\u001b[33m\"\u001b[39m\u001b[33mpeft_config\u001b[39m\u001b[33m\"\u001b[39m: peft_config}, **combined_dict}\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trackio\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspace_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# Add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;28mself\u001b[39m._trackio.config.update(combined_dict, allow_val_change=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/home/marfok/LLM-World/.venv/lib/python3.11/site-packages/trackio/__init__.py:159\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(project, name, group, space_id, space_storage, dataset_id, config, resume, settings, private, embed, auto_log_gpu, gpu_log_interval)\u001b[39m\n\u001b[32m    155\u001b[39m     space_id, dataset_id = utils.preprocess_space_and_dataset_ids(\n\u001b[32m    156\u001b[39m         space_id, dataset_id\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LocalTokenNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalTokenNotFoundError(\n\u001b[32m    160\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou must be logged in to Hugging Face locally when `space_id` is provided to deploy to a Space. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    161\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    162\u001b[39m url = context_vars.current_server.get()\n\u001b[32m    163\u001b[39m share_url = context_vars.current_share_server.get()\n",
      "\u001b[31mLocalTokenNotFoundError\u001b[39m: You must be logged in to Hugging Face locally when `space_id` is provided to deploy to a Space. Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `hf auth login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55823fbd-2d3e-4f48-bd03-9304e650a22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164f59d-bbaa-4bbf-90c6-5cee726d8a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9a1df-b879-4b23-8583-fec86417700d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm-py.311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
